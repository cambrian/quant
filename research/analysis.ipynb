{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy scipy pandas matplotlib numpy_ringbuffer sklearn\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currencies = ['usd', 'btc', 'eth', 'ltc', 'xrp', 'eos']\n",
    "# pairs = [c + '_usd' for c in currencies if c != 'usd']\n",
    "# volume_keys = [c + '_tx_volume' for c in currencies if c != 'usd']\n",
    "\n",
    "# def prep_data(file):\n",
    "#     data = pickle.load(open(file, 'rb'))\n",
    "#     dates = [x['date'] for x in data]\n",
    "#     prices = [{k:v for k,v in x.items() if k in pairs} for x in data]\n",
    "#     volumes = [{(k.partition('_')[0] + '_usd'):v for k,v in x.items() if k in volume_keys} for x in data]\n",
    "#     return {\n",
    "#         'prices': pd.DataFrame(prices, index = dates),\n",
    "#         'volumes': pd.DataFrame(volumes, index = dates)\n",
    "#     }\n",
    "\n",
    "# def reduce_data(data, resampling):\n",
    "#     '''Averages prices, sums volumes'''\n",
    "#     prices = data['prices'].resample(resampling).first().fillna(method='ffill')\n",
    "#     volumes = data['volumes'].resample(resampling).sum().fillna(method='ffill')\n",
    "#     return { 'prices': prices, 'volumes': volumes }\n",
    "\n",
    "# def tail_data(data, n):\n",
    "#     '''get the last n points of the given data'''\n",
    "#     prices = data['prices'].tail(n)\n",
    "#     volumes = data['volumes'].tail(n)\n",
    "#     return { 'prices': prices, 'volumes': volumes }\n",
    "\n",
    "# def viz_data(data):\n",
    "#     '''Only plots prices for now'''\n",
    "#     plt.plot(data['prices'] / data['prices'].mean() - 1)\n",
    "#     plt.show()\n",
    "\n",
    "# def find_gaps(data, freq):\n",
    "#     idx_ref = pd.date_range(start=data.index[0], end=data.index[-1],freq=freq)\n",
    "#     gaps = idx_ref[~idx_ref.isin(data.index)]\n",
    "#     return gaps\n",
    "\n",
    "# data = prep_data('data/data.p')\n",
    "# data_min = reduce_data(prep_data('data/data-minute.p'), '1Min')\n",
    "# data_5min = reduce_data(data_min, '5Min')\n",
    "# data_15min = reduce_data(data_min, '15Min')\n",
    "# viz_data(data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min = pd.read_hdf('data/1min.h5')\n",
    "data_15min = data_min.resample('15Min').first()\n",
    "data_1h = data_min.resample('1h').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.util.stats import Ema, HoltEma\n",
    "price_data = data_15min.apply(lambda x: x['price'])\n",
    "h = HoltEma(64, 512)\n",
    "smooth = price_data.apply(h.step, axis=1).rename(columns=lambda x: str(x) + '_smooth')\n",
    "together = pd.concat([price_data, smooth], axis=1)\n",
    "(together/together.mean()).plot(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HoltEma(3, 3)\n",
    "def htrend(d):\n",
    "    h.step(d)\n",
    "    return h.trend\n",
    "price_data_ = price_data[-300:].filter(regex=\"-LTC-\")\n",
    "trend = price_data_.apply(htrend, axis=1).rename(columns=lambda x: str(x) + '_trend')\n",
    "e = Ema(9)\n",
    "ema_trend = price_data_.diff()[1:].apply(e.step, axis=1).rename(columns=lambda x: str(x) + '_ema_trend')\n",
    "h2 = HoltEma(9, 9)\n",
    "hema_trend = price_data_.diff()[1:].apply(h2.step, axis=1).rename(columns=lambda x: str(x) + '_hema_trend')\n",
    "(price_data_ / price_data_.mean()).plot(figsize=(16,10))\n",
    "together = pd.concat([trend, ema_trend, hema_trend], axis=1)\n",
    "((together - together.mean()) / together.std()).plot(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.util.types import TradingPair, ExchangePair\n",
    "from trader.util.constants import LTC, USDT, BINANCE\n",
    "price_data_ = price_data[-100:].filter(regex=\"-LTC-\")\n",
    "diffs = price_data_.diff()[1:]\n",
    "diffs /= diffs.std()\n",
    "x = diffs.abs().sort_values(by=ExchangePair(BINANCE, TradingPair(LTC, USDT))).reset_index(drop=True)\n",
    "diffs.plot()\n",
    "x.plot()\n",
    "price_data_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_matrix(df,size=10):\n",
    "#     '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "#     Input:\n",
    "#         df: pandas DataFrame\n",
    "#         size: vertical and horizontal size of the plot'''\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(size, size))\n",
    "#     ax.matshow(df)\n",
    "#     plt.xticks(range(len(df.columns)), df.columns);\n",
    "#     plt.yticks(range(len(df.index)), df.index);\n",
    "#     # Loop over data dimensions and create text annotations.\n",
    "#     for i in range(len(df.index)):\n",
    "#         for j in range(len(df.columns)):\n",
    "#             ax.text(j, i, '{:0.2f}'.format(df.iloc[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "# plot_matrix(data_15min['prices'].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_correlate_(x, y):\n",
    "#     return np.argmax(np.correlate(x, y, mode='full')) - len(x) + 1\n",
    "\n",
    "# def cross_correlate(df):\n",
    "#     '''Compute cross-correlation matrix for the given dataframe.'''\n",
    "#     ccs = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "#     for i in df.columns:\n",
    "#         for j in df.columns:\n",
    "#             if i == j:\n",
    "#                 ccs.loc[i,j] = 0\n",
    "#                 continue\n",
    "#             if np.isnan(ccs.loc[i,j]):\n",
    "#                 ccs.loc[i,j] = cross_correlate_(df[i], df[j])\n",
    "#                 ccs.loc[j,i] = -ccs.loc[i,j]\n",
    "#     return ccs\n",
    "\n",
    "# print(cross_correlate(pd.DataFrame([[1,2],[2,1],[1,2],[2,1],[1,2]])))\n",
    "# print(cross_correlate(pd.DataFrame([[1,1],[2,2],[3,3],[4,4],[5,5]])))\n",
    "    \n",
    "# print(cross_correlate(data_min['prices']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from trader.util.constants import BCH, BTC, EOS, ETH, LTC, NEO, XRP\n",
    "from trader.util.stats import Ema\n",
    "\n",
    "# TODO: fetch this dynamically\n",
    "CIRCULATING_SUPPLY = pd.Series(\n",
    "    {BTC: 18e6, ETH: 106e6, XRP: 42e9, BCH: 18e6, EOS: 913e6, LTC: 62e6, NEO: 65e6}\n",
    ")\n",
    "\n",
    "# TODO\n",
    "# think about how to do this. what if a quote_usd pair does not exist on a particular exchange?\n",
    "def convert_quotes_to_usd(frame):\n",
    "    frame = frame.copy()\n",
    "    frame[\"volume\"] *= frame[\"price\"]\n",
    "    return frame\n",
    "\n",
    "\n",
    "def aggregate_currency_quotes(moving_volumes, frame):\n",
    "    \"\"\"\n",
    "    Aggregate quotes for the same base currency across all exchanges and pairs. Quotes should\n",
    "    already be in USD.\n",
    "\n",
    "    Returns aggregate quotes.\n",
    "\n",
    "    Note: this may cause unwanted fluctuation in the aggregate price if quotes disagree - and\n",
    "    trading volume moves back and forth between them. To combat this we weight quotes by a slow\n",
    "    moving average of trading volume, but is there a better solution?\n",
    "    \"\"\"\n",
    "    currencies = {pair.base for pair in frame.index}\n",
    "    aggregates = pd.DataFrame(index=currencies, columns=frame.columns)\n",
    "    for c in currencies:\n",
    "        components = frame.filter(regex=\"-\" + str(c) + \"-.*\", axis=0)\n",
    "        moving_volumes_c = moving_volumes.filter(regex=\"-\" + str(c) + \"-.*\") + 1e-10\n",
    "        volume = components[\"volume\"].sum()\n",
    "        price = components[\"price\"] @ moving_volumes_c / moving_volumes_c.sum()\n",
    "        aggregates.loc[c] = (price, volume)\n",
    "    return aggregates\n",
    "\n",
    "\n",
    "def add_baskets_mut(baskets, aggregates):\n",
    "    \"\"\"\n",
    "    Compute a cap-weighted basket of currencies. Input should have aggregate quotes.\n",
    "    \"\"\"\n",
    "    for name, currencies in baskets.items():\n",
    "        components = aggregates.loc[currencies]\n",
    "        # scale down to avoid numerical instability\n",
    "        price = components[\"price\"] @ CIRCULATING_SUPPLY[currencies] / 1e9\n",
    "        volume = components[\"volume\"].sum()\n",
    "        aggregates.loc[name] = (price, volume)\n",
    "\n",
    "\n",
    "class SignalAggregator:\n",
    "    \"\"\"\n",
    "    Adds cap-weighted baskets to frame.\n",
    "    TODO: also convert non-USD quotes to USD, add aggregated currency prices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, volume_half_life, baskets):\n",
    "        self.__moving_volumes = Ema(volume_half_life)\n",
    "        self.__baskets = baskets\n",
    "\n",
    "    def step(self, frame):\n",
    "        frame = convert_quotes_to_usd(frame)\n",
    "        moving_volumes = self.__moving_volumes.step(frame[\"volume\"])\n",
    "        aggregates = aggregate_currency_quotes(moving_volumes, frame)\n",
    "        add_baskets_mut(self.__baskets, aggregates)\n",
    "        return aggregates.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trader.util.stats import HoltEma\n",
    "from trader.util.maths import sigmoid\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "\n",
    "\n",
    "class ExecutionStrategy:\n",
    "    def __init__(self, size, window_size, trend_hl, accel_hl, trend_cutoff, min_edge, min_edge_to_close):\n",
    "        self.size = size\n",
    "        # Does it make sense to replace with something else? (like % of takes that are buys)\n",
    "        self.trend_estimator = HoltEma(trend_hl, accel_hl)\n",
    "        self.mvmt_history = None\n",
    "        self.prev_prices = None\n",
    "        self.window_size = window_size - 1\n",
    "        self.trend_cutoff = trend_cutoff\n",
    "        self.min_edge = min_edge\n",
    "        self.min_edge_to_close = min_edge_to_close\n",
    "\n",
    "    def tick(self, positions, prices, fairs, fees):\n",
    "        \"\"\"Takes fair as Gaussian, positions in base currency.\n",
    "        Returns orders in base currency (negative size indicates sell).\n",
    "\n",
    "        Since our fair estimate may have skewed uncertainty, it may be the case that\n",
    "        a price change for one trading pair causes us to desire positions in other\n",
    "        pairs. Therefore get_orders needs to consider the entire set of fairs and\n",
    "        bid/asks at once.\n",
    "\n",
    "        TODO: generalize to multiple quotes\n",
    "        TODO: use books instead of just the best bid/ask price\n",
    "        \"\"\"\n",
    "        # bookkeepping\n",
    "        if self.mvmt_history is None:\n",
    "            self.mvmt_history = RingBuffer(\n",
    "                self.window_size, dtype=(np.float64, len(prices.index)))\n",
    "            self.prev_prices = prices\n",
    "        mvmt = prices - self.prev_prices\n",
    "        self.prev_prices = prices\n",
    "        self.mvmt_history.append(mvmt)\n",
    "        trend = self.trend_estimator.step(mvmt)\n",
    "        if len(self.mvmt_history) < self.window_size or not self.trend_estimator.ready:\n",
    "            return pd.Series(0, index=prices.index)\n",
    "        mvmt_history = pd.DataFrame(self.mvmt_history, columns=prices.index).std()\n",
    "        \n",
    "        z_edge = (fairs.mean - prices) / fairs.stddev\n",
    "        z_trend = trend / mvmt_history.std()\n",
    "        print(z_trend)\n",
    "#         gradient = fairs.gradient(prices) * fairs.mean\n",
    "#         gradient_direction = gradient / (np.linalg.norm(gradient) + 1e-100)\n",
    "        target_position_values = z_edge * self.size\n",
    "        pair_positions = positions[[(ep.exchange_id, ep.base) for ep in prices.index]].set_axis(\n",
    "            prices.index, inplace=False\n",
    "        )\n",
    "        proposed_orders = target_position_values / fairs.mean - pair_positions\n",
    "        pct_edge = fairs.mean / prices - 1\n",
    "        profitable = np.sign(proposed_orders) * pct_edge > fees + self.min_edge\n",
    "        trending_correctly = z_trend * np.sign(z_edge) > self.trend_cutoff\n",
    "        profitable_orders = proposed_orders * profitable * trending_correctly\n",
    "\n",
    "        should_close = -np.sign(pair_positions) * pct_edge > fees + self.min_edge_to_close\n",
    "        position_closing_orders = -pair_positions * (1 - profitable) * should_close * trending_correctly\n",
    "\n",
    "        return profitable_orders + position_closing_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import numpy as np\n",
    "import pandas as pd\n",
    "from trader.util import Gaussian\n",
    "from abc import ABC, abstractmethod\n",
    "from trader.util.stats import HoltEma\n",
    "\n",
    "# Note: Assumes all orders fill at last trade price. Attempting to simulate market-making would\n",
    "# require combing through book and trade data, which is too much work for us to do at the moment.\n",
    "\n",
    "\n",
    "def execute_orders(fees, prices, balances, orders):\n",
    "    for (ep, size) in orders.items():\n",
    "        value = size * prices[ep]\n",
    "        balances[(ep.exchange_id, ep.quote)] -= value\n",
    "        balances[(ep.exchange_id, ep.quote)] -= abs(value) * fees\n",
    "        balances[(ep.exchange_id, ep.base)] += size\n",
    "\n",
    "\n",
    "def run(\n",
    "    strategy,\n",
    "    data,\n",
    "    aggregator = SignalAggregator(1,{}),\n",
    "    execution_strategy=ExecutionStrategy(1000, 500, 1, 2, 0, 0, 0),\n",
    "    fees=0\n",
    "):\n",
    "    balances = pd.Series(0.0, index=[(ep.exchange_id, ep.base) for ep in data.iloc[0].index])\n",
    "    quote_balances = pd.Series(0.0, index={(ep.exchange_id, ep.quote) for ep in data.iloc[0].index})\n",
    "    balances = balances.append(quote_balances)\n",
    "    balance_history = []\n",
    "    fair_history = []\n",
    "    for frame in data:\n",
    "        signals = aggregator.step(frame)\n",
    "#         print(signals)\n",
    "        fairs = strategy.tick(frame, signals)\n",
    "        fairs &= Gaussian(frame['price'], [1e100 for _ in frame.index])\n",
    "        \n",
    "        orders = execution_strategy.tick(balances, frame['price'], fairs, fees)\n",
    "        execute_orders(fees, frame['price'], balances, orders)\n",
    "\n",
    "        fair_history.append(fairs)\n",
    "        balance_history.append(balances.copy())\n",
    "    return {\n",
    "        'data': data,\n",
    "        'fairs': pd.DataFrame(fair_history, index=data.index),\n",
    "        'balances': pd.DataFrame(balance_history, index=data.index)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import analyze\n",
    "from trader.strategy import Strategy\n",
    "from trader.util import Gaussian\n",
    "\n",
    "class DummyStrategy(Strategy):\n",
    "    \"\"\"A strategy that always returns a null prediction.\"\"\"\n",
    "\n",
    "    def tick(self, frame, signals):\n",
    "        return Gaussian(pd.Series([]), [])\n",
    "\n",
    "\n",
    "analyze(run(DummyStrategy(), data_min.tail(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from trader.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, HoltEma\n",
    "from trader.util import Gaussian\n",
    "from analysis import analyze\n",
    "\n",
    "from trader.util.constants import BCH, BTC, EOS, ETH, LTC, NEO, XRP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import warnings\n",
    "\n",
    "def bhattacharyya_multi(a, b):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of Bhattacharyya distance for 1-d Gaussians.\n",
    "    Formula from https://en.wikipedia.org/wiki/Bhattacharyya_distance\n",
    "    \"\"\"\n",
    "    return (1/4) * (\n",
    "        np.log((1/4) * (a.variance / b.variance + b.variance / a.variance + 2))\n",
    "        + (a.mean - b.mean) ** 2 / (a.variance + b.variance)\n",
    "    )\n",
    "\n",
    "def intersect_with_disagreement(gaussians):\n",
    "    intersection = Gaussian.intersect(gaussians)\n",
    "    disagreement = pd.DataFrame([bhattacharyya_multi(intersection, g) for g in gaussians])\n",
    "    return Gaussian(intersection.mean, intersection.variance * (1 + disagreement.pow(2).sum()))\n",
    "\n",
    "def remove_trend(df):\n",
    "    \"\"\"Remove linear trend from the input data by subtracting the OLS.\"\"\"\n",
    "    A = np.vstack([df.index.values, np.ones(len(df.index))]).T\n",
    "    Q = np.linalg.lstsq(A, df, rcond=None)[0]\n",
    "    trend = A @ Q\n",
    "    return df - trend\n",
    "\n",
    "class KalmanFilter(Strategy):\n",
    "    '''\n",
    "    Models fairs based on correlated movements between pairs. Weights predictions by volume and\n",
    "    likelihood of cointegration.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, window_size, movement_half_life, trend_half_life, cointegration_period, maxlag):\n",
    "        self.price_history = None\n",
    "        self.window_size = window_size\n",
    "        self.movement_half_life = movement_half_life\n",
    "        self.moving_prices = HoltEma(movement_half_life, trend_half_life)\n",
    "        self.moving_signal_volumes = Ema(window_size / 2)\n",
    "        self.cointegration_period = cointegration_period\n",
    "        self.maxlag = maxlag\n",
    "        self.prev_fair = None\n",
    "        self.sample_counter = 0\n",
    "        self.coint_f = None\n",
    "\n",
    "    # the fair combination step assumes that all signals are i.i.d. They are not (and obviously not in the case\n",
    "    # of funds). Is this a problem?\n",
    "    def tick(self, frame, signals):\n",
    "        inputs = pd.concat([frame, signals])\n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(\n",
    "                self.window_size, dtype=(np.float64, len(inputs.index)))\n",
    "\n",
    "        self.price_history.append(inputs['price'])\n",
    "\n",
    "        price_history = pd.DataFrame(self.price_history, columns=inputs.index)\n",
    "\n",
    "        moving_prices = self.moving_prices.step(inputs[\"price\"])\n",
    "        moving_signal_volumes = self.moving_signal_volumes.step(signals[\"volume\"])\n",
    "\n",
    "        if len(self.price_history) < self.window_size or not self.moving_prices.ready:\n",
    "            return Gaussian(pd.Series([]),[])\n",
    "\n",
    "        if self.prev_fair is None:\n",
    "            self.prev_fair = Gaussian(moving_prices, [1e100 for _ in inputs.index])\n",
    "            \n",
    "        if self.coint_f is None:\n",
    "            self.coint_f = pd.DataFrame(1, index=signals.index, columns=inputs.index)\n",
    "            \n",
    "        # The moving average is already trend-compenstated, so we remove trend from the data.\n",
    "        price_history = remove_trend(price_history)\n",
    "\n",
    "        # calculate p values for pair cointegration\n",
    "        if self.sample_counter == 0:\n",
    "            for i in signals.index:\n",
    "                for j in inputs.index:\n",
    "                    # ignore collinearity warning\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\")\n",
    "                        p = coint(price_history[i], price_history[j], trend='ct', maxlag=self.maxlag, autolag=None)[1]\n",
    "                        self.coint_f.loc[i,j] = 1 + p * p * p * 15625 # .04 -> 2, .05 -> ~3, .1 -> 15.625\n",
    "        self.sample_counter = (self.sample_counter - 1) % self.cointegration_period\n",
    "        \n",
    "        diffs = price_history.diff()[1:]\n",
    "        var = price_history.var()\n",
    "        stddev = np.sqrt(var) + 1e-100\n",
    "        r = price_history.corr().loc[signals.index]\n",
    "        r2 = r * r\n",
    "        correlated_slopes = r.mul(stddev, axis=1).div(stddev[signals.index], axis=0)\n",
    "        sqrt_volume = np.sqrt(moving_signal_volumes)\n",
    "        # ideally use mkt cap instead of volume?\n",
    "        volume_f = np.max(sqrt_volume) / sqrt_volume\n",
    "#         f = self.coint_f.mul(volume_f, axis=0)\n",
    "        \n",
    "        delta = signals[\"price\"] - moving_prices[signals.index]\n",
    "        fair_delta_means = correlated_slopes.mul(delta, axis=0)\n",
    "        delta_vars = diffs.rolling(self.movement_half_life).sum()[self.movement_half_life:].var()\n",
    "        correlated_delta_vars = np.square(correlated_slopes).mul(delta_vars[signals.index], axis=0)\n",
    "#         fair_delta_vars = f * correlated_delta_vars + (1-r2) * delta_vars\n",
    "        fair_delta_vars = (correlated_delta_vars + (1-r2) * self.coint_f * delta_vars).mul(volume_f, axis=0)\n",
    "        fair_deltas = [Gaussian(fair_delta_means.loc[i], fair_delta_vars.loc[i]) for i in signals.index]\n",
    "        fair_delta = intersect_with_disagreement(fair_deltas)\n",
    "        absolute_fair = fair_delta + moving_prices\n",
    "        \n",
    "        step = signals[\"price\"] - (self.prev_fair.mean + self.moving_prices.trend)[signals.index]\n",
    "        fair_step_means = correlated_slopes.mul(step, axis=0)\n",
    "        step_vars = diffs.var()\n",
    "        correlated_step_vars = np.square(correlated_slopes).mul(step_vars[signals.index], axis=0)\n",
    "#         fair_step_vars = f * correlated_step_vars + (1-r2) * step_vars\n",
    "        fair_step_vars = (correlated_step_vars + (1-r2) * self.coint_f * step_vars).mul(volume_f, axis=0)\n",
    "        fair_steps = [Gaussian(fair_step_means.loc[i], fair_step_vars.loc[i]) for i in signals.index]\n",
    "        fair_step = intersect_with_disagreement(fair_steps)\n",
    "        relative_fair = fair_step + self.prev_fair + self.moving_prices.trend\n",
    "        \n",
    "#         fair = intersect_with_disagreement(fair_deltas + fair_steps)\n",
    "        fair = intersect_with_disagreement([absolute_fair, relative_fair])\n",
    "#         print(\"\\n\\nASDSA\")\n",
    "#         print(frame[\"price\"])\n",
    "#         print(absolute_fair)\n",
    "#         print(relative_fair)\n",
    "#         print(fair)\n",
    "#         assert(0)\n",
    "        self.prev_fair = fair\n",
    "        return fair[frame.index]\n",
    "\n",
    "analyze(run(\n",
    "    KalmanFilter(window_size = 7500, movement_half_life = 90, trend_half_life = 3000, cointegration_period=60, maxlag = 120),\n",
    "    data_min.tail(15000),\n",
    "    SignalAggregator(7500, { \"total_market\": [BTC, ETH, XRP, LTC, EOS, NEO] }),\n",
    "    ExecutionStrategy(1000, 7500, 1, 3, -.05, .002, .0005),\n",
    "    fees=0.00075,\n",
    "))\n",
    "\n",
    "# analyze(run(\n",
    "#     KalmanFilter(window_size = 500, movement_half_life = 6, trend_half_life = 256, cointegration_period=32, maxlag = 8),\n",
    "#     data_15min,\n",
    "# #     data_15min.tail(1500),\n",
    "#     SignalAggregator(500, { \"total_market\": [BTC, ETH, XRP, LTC, EOS, NEO] }),\n",
    "#     ExecutionStrategy(1000, 500, 1, 3, -.5, .001, .0005),\n",
    "#     fees=0.00075,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "import pickle\n",
    "\n",
    "from research.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, Gaussian\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from trader.util.linalg import orthogonal_projection, hyperplane_projection\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def johansen(P, maxlag):\n",
    "    mean = P.mean()\n",
    "    c = coint_johansen(P / mean - 1, det_order=-1, k_ar_diff=maxlag)\n",
    "    significant_results = (c.lr1 > c.cvt[:,1]) * (c.lr2 < c.cvm[:,2])\n",
    "    Q = pd.DataFrame(c.evec[:,significant_results].T, columns=P.columns)\n",
    "    return mean, Q.div(Q.apply(np.linalg.norm, axis=1), axis=0)\n",
    "\n",
    "# TODO: reimplement with vector autoregression test, similar to johansen internals.\n",
    "def test_coint(P, mean_train, q, maxlag):\n",
    "    x = (P / mean_train - 1) @ q\n",
    "    return adfuller(x, regression=\"nc\", maxlag=maxlag, autolag=None)[1]\n",
    "\n",
    "def cosine_similar_to_any(Q, x):\n",
    "    for q in Q:\n",
    "        if distance.cosine(q, x) < 0.1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class LiveCointegrator(Strategy):\n",
    "    def __init__(self, train_size, validation_size, cointegration_period, maxlag):\n",
    "        self.window_size = train_size + validation_size\n",
    "        self.train_size = train_size\n",
    "        self.validation_size = validation_size\n",
    "        self.cointegration_period = cointegration_period\n",
    "        self.maxlag = maxlag\n",
    "        self.sample_counter = 0\n",
    "        self.price_history = None\n",
    "        self.Q = None\n",
    "        self.base_prices = None\n",
    "#         self.covs = None\n",
    "        self.base_cov = None\n",
    "        self.prev_fair = None\n",
    "        self.coints = []\n",
    "\n",
    "    def step(self, frame):\n",
    "        prices = frame[\"price\"]\n",
    "        \n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(self.window_size, dtype=(np.float64, len(prices.index)))\n",
    "            \n",
    "        if self.prev_fair is None:\n",
    "            self.prev_fair = self.null_estimate(prices)\n",
    "            \n",
    "        self.price_history.append(prices)\n",
    "        \n",
    "        if len(self.price_history) < self.window_size:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        if self.sample_counter == 0:\n",
    "            P = pd.DataFrame(self.price_history, columns=prices.index)\n",
    "            P_train = P.iloc[:self.train_size]\n",
    "            P_val = P.iloc[self.train_size:]\n",
    "            mean, candidates = johansen(P_train, self.maxlag)\n",
    "            self.base_prices = mean\n",
    "            new_coints = [q for q in candidates.values if test_coint(P_val, mean, q, self.maxlag) < .05]\n",
    "            old_coints = [q for q in self.coints if test_coint(P_val, mean, q, self.maxlag) < .05 and not cosine_similar_to_any(new_coints, q)]\n",
    "            self.coints = new_coints + old_coints\n",
    "            self.base_cov = (P / P.mean()).cov()\n",
    "            \n",
    "        self.sample_counter -= 1\n",
    "        self.sample_counter %= self.cointegration_period\n",
    "        \n",
    "        if not self.coints:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        fair_means = [hyperplane_projection(prices / self.base_prices - 1, q) for q in self.coints]\n",
    "        fair = Gaussian.intersect([(Gaussian(mean, self.base_cov) + 1) * self.base_prices for mean in fair_means])\n",
    "        \n",
    "#         step_fair_means = [hyperplane_projection(prices / self.prev_fair.mean - 1, q) for q in self.coints]\n",
    "#         step_fair = Gaussian.intersect([Gaussian(mean, self.base_cov) * self.prev_fair.mean for mean in step_fair_means])\n",
    "        \n",
    "#         fair = fair & (self.prev_fair + step_fair)\n",
    "        self.prev_fair = fair\n",
    "        return fair\n",
    "\n",
    "analyze(run(LiveCointegrator(train_size=500, validation_size = 100, cointegration_period=64, maxlag=8), data_15min, fees=0.001, min_edge=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "import pickle\n",
    "\n",
    "from research.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, Gaussian\n",
    "from trader.util.linalg import orthogonal_projection, hyperplane_projection\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "def plot_gaussian(x, y, gaussian):\n",
    "    z = gaussian.pdf(np.dstack(np.meshgrid(x,y)))\n",
    "    plt.pcolormesh(x, y, z)\n",
    "    plt.show()\n",
    "\n",
    "class LivePairCointegrator(Strategy):\n",
    "    def __init__(self, window_size, cointegration_frequency=4):\n",
    "        self.window_size = window_size\n",
    "        self.cointegration_period = window_size // cointegration_frequency\n",
    "        self.sample_counter = 0\n",
    "        self.price_history = None\n",
    "        self.prediction_covs = {}\n",
    "\n",
    "    def step(self, frame):\n",
    "        prices = frame[\"price\"]\n",
    "        \n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(self.window_size, dtype=(np.float64, len(prices.index)))\n",
    "            \n",
    "        self.price_history.append(prices)\n",
    "        \n",
    "        if len(self.price_history) < self.window_size:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        df = pd.DataFrame(self.price_history, columns=prices.index)\n",
    "        self.mean = df.mean()\n",
    "        deltas = df / self.mean - 1\n",
    "        stddev = deltas.std() + 1e-100\n",
    "        regression_slope = deltas.corr().mul(stddev, axis=1).div(stddev, axis=0)\n",
    "        cov = deltas.cov()\n",
    "        \n",
    "        if self.sample_counter == 0:\n",
    "            \n",
    "            for pair_i in prices.index:\n",
    "                for pair_j in prices.index:\n",
    "                    if pair_i >= pair_j:\n",
    "                        continue\n",
    "                    deltas_ij = deltas[[pair_i, pair_j]]\n",
    "                    regression_vector = [regression_slope.loc[pair_i][pair_j], -1]\n",
    "                    residuals = orthogonal_projection(deltas_ij, regression_vector)\n",
    "                    cov_resid = residuals.cov()\n",
    "                    cov_pred = (deltas_ij - residuals).cov()\n",
    "                    print(cov_pred)\n",
    "                    # We can calculate the cointegration p-value more efficiently by reusing\n",
    "                    # the residuals found via the orthogonal projection but getting it right is\n",
    "                    # tricky. So we just do it the dumb way\n",
    "                    p = coint(deltas[pair_j], deltas[pair_i], trend='nc', maxlag=0, autolag=None)[1]\n",
    "#                     # regularize prediction covariance by using the raw price covariance shape with the \n",
    "#                     # prediction covariance slope\n",
    "#                     cov_sam = deltas_ij.cov()\n",
    "#                     w_pred, v_pred = np.linalg.eigh(cov_pred)\n",
    "#                     w_sam, v_sam = np.linalg.eigh(cov_sam)\n",
    "#                     eigenvalue_ratio_sam = w_sam[1] / w_sam[0]\n",
    "#                     w_reg = w_pred[1] * np.array([eigenvalue_ratio_sam, 1])\n",
    "#                     cov_reg = v_pred @ np.diag(w_reg) @ v_pred.T\n",
    "#                     # discount predictions based on p-value\n",
    "#                     cov_reg *= max(1, p * p * 900) # min(1e4, np.cosh(16 * p))\n",
    "#                     cov_reg = pd.DataFrame(cov_reg, index=deltas_ij.columns, columns=deltas_ij.columns)\n",
    "                    cov_reg = cov_pred + max(1, p * p * 900) * cov_resid\n",
    "                    self.prediction_covs[pair_i, pair_j] = cov_reg\n",
    "                    self.prediction_covs[pair_j, pair_i] = cov_reg.loc[::-1, ::-1]\n",
    "                    \n",
    "                    # uncomment and run this if you want to see what the prediction covariances look like\n",
    "                    \n",
    "                    prediction = Gaussian(pd.Series(0, index=deltas_ij.columns), cov_resid)\n",
    "                    base = Gaussian(pd.Series(0, index=deltas_ij.columns),  cov_pred)\n",
    "                    regularized = Gaussian(pd.Series(0, index=deltas_ij.columns), cov_reg)\n",
    "                    xrange = deltas[pair_i].max() - deltas[pair_i].min()\n",
    "                    yrange = deltas[pair_j].max() - deltas[pair_j].min()\n",
    "                    x = np.linspace(-xrange, xrange)\n",
    "                    y = np.linspace(-yrange, yrange)\n",
    "                    plot_gaussian(x, y, prediction)\n",
    "                    plot_gaussian(x, y, base)\n",
    "                    plot_gaussian(x, y, regularized)\n",
    "            \n",
    "        self.sample_counter -= 1\n",
    "        self.sample_counter %= self.cointegration_period\n",
    "        \n",
    "        fairs = []\n",
    "        delta = prices / self.mean - 1\n",
    "        cov = deltas.cov()\n",
    "        for pair_i in prices.index:\n",
    "            for pair_j in prices.index:\n",
    "                if pair_i >= pair_j:\n",
    "                    continue\n",
    "                regression_vector = [regression_slope.loc[pair_i][pair_j], -1]\n",
    "#                 residuals = orthogonal_projection(deltas[[pair_i, pair_j]], regression_vector)\n",
    "                fair_delta_mean = hyperplane_projection(delta[[pair_i, pair_j]], regression_vector)\n",
    "                fair_cov = self.prediction_covs[pair_i, pair_j]\n",
    "                fair = Gaussian(fair_delta_mean, fair_cov)\n",
    "#                 fair = Gaussian(fair_delta_mean, cov.loc[[pair_i, pair_j], [pair_i, pair_j]])\n",
    "#                 scale = fair.z_score(delta[[pair_i, pair_j]])\n",
    "#                 fair = Gaussian(fair.mean, fair.covariance * scale)\n",
    "#                 print(prices)\n",
    "#                 print(fair)\n",
    "#                 x = np.linspace(deltas[pair_i].min(), deltas[pair_i].max()) - deltas[pair_i].mean()\n",
    "#                 y = np.linspace(deltas[pair_j].min(), deltas[pair_j].max()) - deltas[pair_j].mean()\n",
    "#                 plot_gaussian(x, y, fair)\n",
    "#                 plot_gaussian(x, y, Gaussian(pd.Series(0, index=[pair_i, pair_j]), residuals.cov()))\n",
    "#                 plot_gaussian(x, y, Gaussian(pd.Series(0, index=[pair_i, pair_j]), deltas[[pair_i, pair_j]].cov()))\n",
    "                fairs.append(fair)\n",
    "#         print(fairs)\n",
    "        fair = (Gaussian.intersect(fairs) + 1) * self.mean\n",
    "#         print(prices)\n",
    "#         print(fair)\n",
    "#         x = np.linspace(df['BTC_USDT'].min(), df['BTC_USDT'].max())\n",
    "#         y = np.linspace(df['ETH_USDT'].min(), df['ETH_USDT'].max())\n",
    "#         plot_gaussian(x, y, fair[['BTC_USDT', 'ETH_USDT']])\n",
    "#         print(fair / prices - 1)\n",
    "        return fair\n",
    "\n",
    "analyze(run(LivePairCointegrator(window_size=96), data_15min.tail(1500), fees=0.001, min_edge=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CointegratorStrategy(cointegration_window_size = 16), data_15min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(KalmanFilterStrategy(\n",
    "    correlation_window_size = 480,\n",
    "    movement_half_life = 1\n",
    "), tail_data(data_min, 10000), fees = 0.002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategy import CombinedStrategy\n",
    "\n",
    "analyze(run(CombinedStrategy([\n",
    "    KalmanFilterStrategy(correlation_window_size = 60, movement_half_life = 3),\n",
    "    CointegratorStrategy(cointegration_window_size = 16)\n",
    "]), data_15min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CointegratorStrategy(cointegration_window_size = 512), data_5min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(KalmanFilterStrategy(correlation_window_size = 165, movement_half_life = 70), data_5min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CombinedStrategy([\n",
    "    KalmanFilterStrategy(correlation_window_size = 16, movement_half_life = 8),\n",
    "    CointegratorStrategy(cointegration_window_size = 64)\n",
    "]), tail_data(data_min, 1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def find_best_window_sizes(data, n):\n",
    "    points = []\n",
    "    best = None\n",
    "    best_ror = 0\n",
    "    for _ in range(n):\n",
    "        movement_half_life = random.expovariate(1) * 15\n",
    "#         window_ratio = random.uniform(1, 10)\n",
    "#         window_size = max(3, int(movement_half_life * window_ratio))\n",
    "#         movement_half_life = 2\n",
    "        window_size = int(random.expovariate(1) * 100) + 400\n",
    "#         window_size = int(random.expovariate(1) * 30) + 3\n",
    "#         window_size = 4\n",
    "#         window_size = 32\n",
    "        print('Trying window_size: {0} and half_life: {1}'.format(window_size, movement_half_life))\n",
    "        ror = analyze(run(KalmanFilter(window_size, movement_half_life), data, fees = 0.002), plot=False)\n",
    "        print('  RoR: {0}'.format(ror))\n",
    "        point = { 'window_size': window_size, 'half_life': movement_half_life, 'RoR': ror }\n",
    "        points.append(point)\n",
    "        if ror > best_ror:\n",
    "            best = point\n",
    "            best_ror = ror\n",
    "    print('Best found:')\n",
    "    print(best)\n",
    "    pd.DataFrame(points).plot.scatter('window_size', 'half_life', c='RoR', colormap='jet')\n",
    "    \n",
    "find_best_window_sizes(tail_data(data_min, 1500), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
