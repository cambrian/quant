{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy scipy pandas matplotlib numpy_ringbuffer sklearn\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currencies = ['usd', 'btc', 'eth', 'ltc', 'xrp', 'eos']\n",
    "# pairs = [c + '_usd' for c in currencies if c != 'usd']\n",
    "# volume_keys = [c + '_tx_volume' for c in currencies if c != 'usd']\n",
    "\n",
    "# def prep_data(file):\n",
    "#     data = pickle.load(open(file, 'rb'))\n",
    "#     dates = [x['date'] for x in data]\n",
    "#     prices = [{k:v for k,v in x.items() if k in pairs} for x in data]\n",
    "#     volumes = [{(k.partition('_')[0] + '_usd'):v for k,v in x.items() if k in volume_keys} for x in data]\n",
    "#     return {\n",
    "#         'prices': pd.DataFrame(prices, index = dates),\n",
    "#         'volumes': pd.DataFrame(volumes, index = dates)\n",
    "#     }\n",
    "\n",
    "# def reduce_data(data, resampling):\n",
    "#     '''Averages prices, sums volumes'''\n",
    "#     prices = data['prices'].resample(resampling).first().fillna(method='ffill')\n",
    "#     volumes = data['volumes'].resample(resampling).sum().fillna(method='ffill')\n",
    "#     return { 'prices': prices, 'volumes': volumes }\n",
    "\n",
    "# def tail_data(data, n):\n",
    "#     '''get the last n points of the given data'''\n",
    "#     prices = data['prices'].tail(n)\n",
    "#     volumes = data['volumes'].tail(n)\n",
    "#     return { 'prices': prices, 'volumes': volumes }\n",
    "\n",
    "# def viz_data(data):\n",
    "#     '''Only plots prices for now'''\n",
    "#     plt.plot(data['prices'] / data['prices'].mean() - 1)\n",
    "#     plt.show()\n",
    "\n",
    "# def find_gaps(data, freq):\n",
    "#     idx_ref = pd.date_range(start=data.index[0], end=data.index[-1],freq=freq)\n",
    "#     gaps = idx_ref[~idx_ref.isin(data.index)]\n",
    "#     return gaps\n",
    "\n",
    "# data = prep_data('data/data.p')\n",
    "# data_min = reduce_data(prep_data('data/data-minute.p'), '1Min')\n",
    "# data_5min = reduce_data(data_min, '5Min')\n",
    "# data_15min = reduce_data(data_min, '15Min')\n",
    "# viz_data(data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min = pd.read_hdf('data/1min.h5')\n",
    "data_15min = data_min.resample('15Min').first()\n",
    "data_1h = data_min.resample('1h').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.util.stats import Ema, HoltEma\n",
    "price_data = data_15min.apply(lambda x: x['price'])\n",
    "h = HoltEma(64, 512)\n",
    "smooth = price_data.apply(h.step, axis=1).rename(columns=lambda x: x + '_smooth')\n",
    "together = pd.concat([price_data, smooth], axis=1)\n",
    "(together/together.mean()).plot(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_matrix(df,size=10):\n",
    "#     '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "#     Input:\n",
    "#         df: pandas DataFrame\n",
    "#         size: vertical and horizontal size of the plot'''\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(size, size))\n",
    "#     ax.matshow(df)\n",
    "#     plt.xticks(range(len(df.columns)), df.columns);\n",
    "#     plt.yticks(range(len(df.index)), df.index);\n",
    "#     # Loop over data dimensions and create text annotations.\n",
    "#     for i in range(len(df.index)):\n",
    "#         for j in range(len(df.columns)):\n",
    "#             ax.text(j, i, '{:0.2f}'.format(df.iloc[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "# plot_matrix(data_15min['prices'].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_correlate_(x, y):\n",
    "#     return np.argmax(np.correlate(x, y, mode='full')) - len(x) + 1\n",
    "\n",
    "# def cross_correlate(df):\n",
    "#     '''Compute cross-correlation matrix for the given dataframe.'''\n",
    "#     ccs = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "#     for i in df.columns:\n",
    "#         for j in df.columns:\n",
    "#             if i == j:\n",
    "#                 ccs.loc[i,j] = 0\n",
    "#                 continue\n",
    "#             if np.isnan(ccs.loc[i,j]):\n",
    "#                 ccs.loc[i,j] = cross_correlate_(df[i], df[j])\n",
    "#                 ccs.loc[j,i] = -ccs.loc[i,j]\n",
    "#     return ccs\n",
    "\n",
    "# print(cross_correlate(pd.DataFrame([[1,2],[2,1],[1,2],[2,1],[1,2]])))\n",
    "# print(cross_correlate(pd.DataFrame([[1,1],[2,2],[3,3],[4,4],[5,5]])))\n",
    "    \n",
    "# print(cross_correlate(data_min['prices']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from trader.util.constants import BCH, BTC, EOS, ETH, LTC, NEO, XRP\n",
    "from trader.util.stats import Ema\n",
    "\n",
    "# TODO: fetch this dynamically\n",
    "CIRCULATING_SUPPLY = pd.Series(\n",
    "    {\"BTC\": 18e6, \"ETH\": 106e6, \"XRP\": 42e9, \"BCH\": 18e6, \"EOS\": 913e6, \"LTC\": 62e6, \"NEO\": 65e6}\n",
    ")\n",
    "\n",
    "# TODO\n",
    "# think about how to do this. what if a quote_usd pair does not exist on a particular exchange?\n",
    "def convert_quotes_to_usd(frame):\n",
    "    frame = frame.copy()\n",
    "    frame[\"volume\"] *= frame[\"price\"]\n",
    "    return frame\n",
    "\n",
    "\n",
    "def aggregate_currency_quotes(moving_volumes, frame):\n",
    "    \"\"\"\n",
    "    Aggregate quotes for the same base currency across all exchanges and pairs. Quotes should\n",
    "    already be in USD.\n",
    "\n",
    "    Returns aggregate quotes.\n",
    "\n",
    "    Note: this may cause unwanted fluctuation in the aggregate price if quotes disagree - and\n",
    "    trading volume moves back and forth between them. To combat this we weight quotes by a slow\n",
    "    moving average of trading volume, but is there a better solution?\n",
    "    \"\"\"\n",
    "    currencies = {pair[:3] for pair in frame.index}\n",
    "    aggregates = pd.DataFrame(index=currencies, columns=frame.columns)\n",
    "    for c in currencies:\n",
    "        components = frame.filter(regex=\"-\" + c + \"-.*\", axis=0)\n",
    "        moving_volumes_c = moving_volumes.filter(regex=\"-\" + c + \"-.*\") + 1e-10\n",
    "        volume = components[\"volume\"].sum()\n",
    "        price = components[\"price\"] @ moving_volumes_c / moving_volumes_c.sum()\n",
    "        aggregates.loc[c] = (price, volume)\n",
    "    return aggregates\n",
    "\n",
    "\n",
    "def add_baskets_mut(baskets, aggregates):\n",
    "    \"\"\"\n",
    "    Compute a cap-weighted basket of currencies. Input should have aggregate quotes.\n",
    "    \"\"\"\n",
    "    for name, currencies in baskets.items():\n",
    "        components = aggregates.loc[currencies]\n",
    "        # scale down to avoid numerical instability\n",
    "        price = components[\"price\"] @ CIRCULATING_SUPPLY[currencies] / 1e9\n",
    "        volume = components[\"volume\"].sum()\n",
    "        aggregates.loc[name] = (price, volume)\n",
    "\n",
    "\n",
    "class SignalAggregator:\n",
    "    \"\"\"\n",
    "    Adds cap-weighted baskets to frame.\n",
    "    TODO: also convert non-USD quotes to USD, add aggregated currency prices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, volume_half_life, baskets):\n",
    "        self.__moving_volumes = Ema(volume_half_life)\n",
    "        self.__baskets = baskets\n",
    "\n",
    "    def step(self, frame):\n",
    "        frame = convert_quotes_to_usd(frame)\n",
    "        moving_volumes = self.__moving_volumes.step(frame[\"volume\"])\n",
    "        aggregates = aggregate_currency_quotes(moving_volumes, frame)\n",
    "        add_baskets_mut(self.__baskets, aggregates)\n",
    "        return aggregates.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import numpy as np\n",
    "import pandas as pd\n",
    "from trader.util import Gaussian\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Note: Assumes all orders fill at last trade price. Attempting to simulate market-making would\n",
    "# require combing through book and trade data, which is too much work for us to do at the moment.\n",
    "\n",
    "def data_currencies(data):\n",
    "    quote_currency = data.iloc[0].index[0].partition(\"_\")[2]\n",
    "    currencies = [quote_currency]\n",
    "    for pair in data.iloc[0].index:\n",
    "        currencies.append(pair.partition('_')[0])\n",
    "    return currencies\n",
    "\n",
    "def to_usd(fairs, x):\n",
    "    \"pairs in x may be any XXX_YYY, so long as YYY_USD is also in fairs.\"\n",
    "\n",
    "def get_orders(balances, prices, fairs, size, fees, min_edge, min_edge_to_close=None):\n",
    "    '''Given current balances, prices, and fair estimates, determine which orders to place.\n",
    "    Assumes all pairs are XXX_USD.\n",
    "    `fairs` should be a Gaussian type. '''\n",
    "    quote_currency = prices.index[0].rpartition(\"_\")[0]\n",
    "    \n",
    "    gradient = fairs.gradient(prices) * fairs.mean\n",
    "    edge = fairs.mean / prices - 1\n",
    "    balance_direction_vector = gradient / (np.linalg.norm(gradient) + 1e-100)\n",
    "    target_balance_values = balance_direction_vector * fairs.z_score(prices) * size\n",
    "    pair_balances = balances.drop([quote_currency]).rename(lambda c: '{}_{}'.format(c, quote_currency))\n",
    "    proposed_orders = target_balance_values / prices - pair_balances\n",
    "    profitable = np.sign(proposed_orders) * edge > fees + min_edge\n",
    "    profitable_orders = proposed_orders * profitable\n",
    "    \n",
    "    if min_edge_to_close is None:\n",
    "        min_edge_to_close = min_edge\n",
    "        \n",
    "    should_close = -np.sign(pair_balances) * edge > fees + min_edge_to_close\n",
    "    balance_closing_orders = -pair_balances * (1 - profitable) * should_close\n",
    "\n",
    "    return profitable_orders + balance_closing_orders\n",
    "\n",
    "\n",
    "def execute_orders(fees, prices, balances, orders):\n",
    "    for (pair, size) in orders.items():\n",
    "        currency, quote_currency = pair.split(\"_\")\n",
    "        value = size * prices[pair]\n",
    "        balances[quote_currency] -= value\n",
    "        balances[quote_currency] -= abs(value) * fees\n",
    "        balances[currency] += size\n",
    "\n",
    "\n",
    "def run(strategy, data, aggregator = SignalAggregator(1,{}), size=1000, fees=0, min_edge = 0, min_edge_to_close = None):\n",
    "    balances = pd.Series(dict.fromkeys(data_currencies(data), 0.))\n",
    "    balances_ = []\n",
    "    fairs_ = []\n",
    "    for frame in data:\n",
    "        signals = aggregator.step(frame)\n",
    "#         print(signals)\n",
    "        fairs = strategy.tick(frame, signals)\n",
    "        fairs &= Gaussian(frame['price'], [1e100 for _ in frame.index])\n",
    "        \n",
    "        orders = get_orders(balances, frame['price'], fairs, size, fees, min_edge, min_edge_to_close)\n",
    "        execute_orders(fees, frame['price'], balances, orders)\n",
    "\n",
    "        fairs_.append(fairs)\n",
    "        balances_.append(balances.copy())\n",
    "    return {\n",
    "        'data': data,\n",
    "        'fairs': pd.DataFrame(fairs_, index=data.index),\n",
    "        'balances': pd.DataFrame(balances_, index=data.index)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import analyze\n",
    "from trader.strategy import Strategy\n",
    "from trader.util import Gaussian\n",
    "\n",
    "class DummyStrategy(Strategy):\n",
    "    \"\"\"A strategy that always returns a null prediction.\"\"\"\n",
    "\n",
    "    def tick(self, frame, signals):\n",
    "        return Gaussian(pd.Series([]), [])\n",
    "\n",
    "\n",
    "analyze(run(DummyStrategy(), data_min.tail(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, HoltEma\n",
    "from trader.util import Gaussian\n",
    "from analysis import analyze\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import warnings\n",
    "\n",
    "def bhattacharyya_multi(a, b):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of Bhattacharyya distance for 1-d Gaussians.\n",
    "    Formula from https://en.wikipedia.org/wiki/Bhattacharyya_distance\n",
    "    \"\"\"\n",
    "    return (1/4) * (\n",
    "        np.log((1/4) * (a.variance / b.variance + b.variance / a.variance + 2))\n",
    "        + (a.mean - b.mean) ** 2 / (a.variance + b.variance)\n",
    "    )\n",
    "\n",
    "def intersect_with_disagreement(gaussians):\n",
    "    intersection = Gaussian.intersect(gaussians)\n",
    "    disagreement = pd.DataFrame([bhattacharyya_multi(intersection, g) for g in gaussians])\n",
    "    return Gaussian(intersection.mean, intersection.variance * (1 + disagreement.pow(2).sum()))\n",
    "\n",
    "def remove_trend(df):\n",
    "    \"\"\"Remove linear trend from the input data by subtracting the OLS.\"\"\"\n",
    "    A = np.vstack([df.index.values, np.ones(len(df.index))]).T\n",
    "    Q = np.linalg.lstsq(A, df, rcond=None)[0]\n",
    "    trend = A @ Q\n",
    "    return df - trend\n",
    "\n",
    "class KalmanFilter(Strategy):\n",
    "    '''\n",
    "    Models fairs based on correlated movements between pairs. Weights predictions by volume and\n",
    "    likelihood of cointegration.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, window_size, movement_half_life, trend_half_life, cointegration_period, maxlag):\n",
    "        self.price_history = None\n",
    "        self.window_size = window_size\n",
    "        self.movement_half_life = movement_half_life\n",
    "        self.moving_prices = HoltEma(movement_half_life, trend_half_life)\n",
    "        self.moving_signal_volumes = Ema(window_size / 2)\n",
    "        self.cointegration_period = cointegration_period\n",
    "        self.maxlag = maxlag\n",
    "        self.prev_fair = None\n",
    "        self.sample_counter = 0\n",
    "        self.coint_f = None\n",
    "\n",
    "    # the fair combination step assumes that all signals are i.i.d. They are not (and obviously not in the case\n",
    "    # of funds). Is this a problem?\n",
    "    def tick(self, frame, signals):\n",
    "        inputs = pd.concat([frame, signals])\n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(\n",
    "                self.window_size, dtype=(np.float64, len(inputs.index)))\n",
    "\n",
    "        self.price_history.append(inputs['price'])\n",
    "\n",
    "        price_history = pd.DataFrame(self.price_history, columns=inputs.index)\n",
    "\n",
    "        moving_prices = self.moving_prices.step(inputs[\"price\"])\n",
    "        moving_signal_volumes = self.moving_signal_volumes.step(signals[\"volume\"])\n",
    "\n",
    "        if len(self.price_history) < self.window_size or not self.moving_prices.ready:\n",
    "            return Gaussian(pd.Series([]),[])\n",
    "\n",
    "        if self.prev_fair is None:\n",
    "            self.prev_fair = Gaussian(moving_prices, [1e100 for _ in inputs.index])\n",
    "            \n",
    "        if self.coint_f is None:\n",
    "            self.coint_f = pd.DataFrame(1, index=signals.index, columns=inputs.index)\n",
    "            \n",
    "        # The moving average is already trend-compenstated, so we remove trend from the data.\n",
    "        price_history = remove_trend(price_history)\n",
    "\n",
    "        # calculate p values for pair cointegration\n",
    "        if self.sample_counter == 0:\n",
    "            for i in signals.index:\n",
    "                for j in inputs.index:\n",
    "                    # ignore collinearity warning\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\")\n",
    "                        p = coint(price_history[i], price_history[j], trend='ct', maxlag=self.maxlag, autolag=None)[1]\n",
    "                        self.coint_f.loc[i,j] = 1 + p * p * p * 15625 # .04 -> 2, .05 -> ~3, .1 -> 15.625\n",
    "        self.sample_counter = (self.sample_counter - 1) % self.cointegration_period\n",
    "        \n",
    "        diffs = price_history.diff()[1:]\n",
    "        var = price_history.var()\n",
    "        stddev = np.sqrt(var) + 1e-100\n",
    "        r = price_history.corr().loc[signals.index]\n",
    "        r2 = r * r\n",
    "        correlated_slopes = r.mul(stddev, axis=1).div(stddev[signals.index], axis=0)\n",
    "        sqrt_volume = np.sqrt(moving_signal_volumes)\n",
    "        volume_f = np.max(sqrt_volume) / sqrt_volume\n",
    "        f = self.coint_f.mul(volume_f, axis=0)\n",
    "        \n",
    "        delta = signals[\"price\"] - moving_prices[signals.index]\n",
    "        fair_delta_means = correlated_slopes.mul(delta, axis=0)\n",
    "        delta_vars = diffs.rolling(self.movement_half_life).sum()[self.movement_half_life:].var()\n",
    "        correlated_delta_vars = np.square(correlated_slopes).mul(delta_vars[signals.index], axis=0)\n",
    "        fair_delta_vars = f * correlated_delta_vars + (1-r2) * delta_vars\n",
    "#         fair_deltas = [Gaussian(fair_delta_means.loc[i], fair_delta_vars.loc[i]) + moving_prices for i in signals.index]\n",
    "        fair_deltas = [Gaussian(fair_delta_means.loc[i], fair_delta_vars.loc[i]) for i in signals.index]\n",
    "        fair_delta = intersect_with_disagreement(fair_deltas)\n",
    "        absolute_fair = fair_delta + moving_prices\n",
    "        \n",
    "        step = signals[\"price\"] - (self.prev_fair.mean + self.moving_prices.trend)[signals.index]\n",
    "        fair_step_means = correlated_slopes.mul(step, axis=0)\n",
    "        step_vars = diffs.var()\n",
    "        correlated_step_vars = np.square(correlated_slopes).mul(step_vars[signals.index], axis=0)\n",
    "        fair_step_vars = f * correlated_step_vars + (1-r2) * step_vars\n",
    "#         fair_steps = [Gaussian(fair_step_means.loc[i], fair_step_vars.loc[i]) + self.prev_fair + self.moving_prices.trend for i in signals.index]\n",
    "        fair_steps = [Gaussian(fair_step_means.loc[i], fair_step_vars.loc[i]) for i in signals.index]\n",
    "        fair_step = intersect_with_disagreement(fair_steps)\n",
    "        relative_fair = fair_step + self.prev_fair + self.moving_prices.trend\n",
    "        \n",
    "        fair = intersect_with_disagreement(fair_deltas + fair_steps)\n",
    "        fair = intersect_with_disagreement([absolute_fair, relative_fair])\n",
    "#         print(\"\\n\\nASDSA\")\n",
    "#         print(frame[\"price\"])\n",
    "#         print(absolute_fair)\n",
    "#         print(relative_fair)\n",
    "#         print(fair)\n",
    "#         assert(0)\n",
    "        self.prev_fair = fair\n",
    "        return fair[frame.index]\n",
    "\n",
    "analyze(run(\n",
    "    KalmanFilter(window_size = 500, movement_half_life = 6, trend_half_life = 256, cointegration_period=32, maxlag = 8),\n",
    "    data_15min,\n",
    "    SignalAggregator(500, { \"total_market\": [\"BTC\", \"ETH\", \"XRP\", \"LTC\", \"EOS\", \"NEO\"] }),\n",
    "    fees=0.00075,\n",
    "    min_edge=0.002,\n",
    "    min_edge_to_close=0.0005\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.strategy import Strategy\n",
    "from trader.util.stats import Ema, HoltEma\n",
    "from trader.util import Gaussian\n",
    "from analysis import analyze\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "# taken from coinmarketcap\n",
    "MARKET_CAPS = pd.Series({ \"BTC\": 99e9, \"ETH\": 18e9, \"XRP\":14e9, \"BCH\": 5e9, \"EOS\": 5e9, \"LTC\": 5e9, \"NEO\": 7e8 })\n",
    "\n",
    "# def add_cap_weighted_fund(P, V, name, currencies):\n",
    "#     \"\"\"\n",
    "#     Calculates a cap-weighted fund of currencies.\n",
    "#     Assumes all inputs are already normalized to the same quote currency.\n",
    "#     \"\"\"\n",
    "#     P_components = pd.DataFrame(index=P.index)\n",
    "#     V_components = pd.DataFrame(index=V.index)\n",
    "#     for c in currencies:\n",
    "#         P_c = P.filter(regex = c+\"_.*\") # there may be multiple pairs for this base currency\n",
    "#         V_c = V.filter(regex = c+\"_.*\")\n",
    "#         V_components[c] = V_c.sum(axis=1)\n",
    "#         P_components[c] = (P_c * V_c).sum(axis=1) / V_components[c]\n",
    "#     P_components.fillna(method='ffill') # avg price will be NaN if volume is 0\n",
    "#     P_components.fillna(method='bfill') # if the first few entries are NaN, backfill. This seems suboptimal\n",
    "#     base_prices = P_components.mean()\n",
    "#     caps = MARKET_CAPS[currencies]\n",
    "#     ratios = 1 / base_prices * caps.values\n",
    "#     P[name] = P_components @ ratios / ratios.sum()\n",
    "#     V[name] = (V_components * P_components).sum(axis=1) / P[name]\n",
    "    \n",
    "\n",
    "def add_cap_weighted_fund(P, volumes, name, pairs):\n",
    "    P_components = P[pairs]\n",
    "    base_prices = P_components.mean()\n",
    "    currencies = [pair[:3] for pair in pairs]\n",
    "    caps = MARKET_CAPS[currencies]\n",
    "    ratios = 1 / base_prices * caps.values\n",
    "    P[name] = P_components @ ratios / ratios.sum()\n",
    "    volumes[name] = volumes[pairs] @ base_prices / P[name].iloc[-1]\n",
    "    \n",
    "def bhattacharyya_multi(a, b):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of Bhattacharyya distance for 1-d Gaussians.\n",
    "    Formula from https://en.wikipedia.org/wiki/Bhattacharyya_distance\n",
    "    \"\"\"\n",
    "    return (1/4) * (\n",
    "        np.log((1/4) * (a.variance / b.variance + b.variance / a.variance + 2))\n",
    "        + (a.mean - b.mean) ** 2 / (a.variance + b.variance)\n",
    "    )\n",
    "\n",
    "class KalmanFilter2(Strategy):\n",
    "    '''\n",
    "    Models fairs based on correlated movements between pairs. Weights predictions by volume and\n",
    "    likelihood of cointegration.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, window_size, movement_half_life, trend_half_life, cointegration_period, maxlag):\n",
    "        self.price_history = None\n",
    "        self.volume_history = None\n",
    "        self.window_size = window_size\n",
    "        self.movement_half_life = movement_half_life\n",
    "        self.moving_prices = HoltEma(movement_half_life, trend_half_life)\n",
    "        self.moving_volumes = Ema(window_size / 2)\n",
    "        self.cointegration_period = cointegration_period\n",
    "        self.maxlag = maxlag\n",
    "        self.prev_fair = None\n",
    "        self.sample_counter = 0\n",
    "        self.coint_f = None\n",
    "\n",
    "    def tick(self, frame):\n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(\n",
    "                self.window_size, dtype=(np.float64, len(frame.index)))\n",
    "#             self.volume_history = RingBuffer(\n",
    "#                 self.window_size, dtype=(np.float64, len(frame.index)))\n",
    "\n",
    "        self.price_history.append(frame['price'])\n",
    "#         self.volume_history.append(frame['volume'])\n",
    "\n",
    "        df = pd.DataFrame(self.price_history, columns=frame.index)\n",
    "#         df_volume = pd.DataFrame(self.volume_history, columns=frame.index)\n",
    "        \n",
    "#         volumes = df_volume.iloc[-1]\n",
    "        volumes = frame[\"volume\"].copy()\n",
    "        add_cap_weighted_fund(df, volumes, \"total_market\", frame.index)\n",
    "#         add_cap_weighted_fund(df, df_volume, \"total_market\", [pair[:3] for pair in frame.index])\n",
    "        prices = df.iloc[-1]\n",
    "#         print(prices)\n",
    "#         print(volumes)\n",
    "        # the fair combination step assumes that all estimates are i.i.d. They are not (and obviously not in the case\n",
    "        # of funds). Is this a problem?\n",
    "\n",
    "        moving_prices = self.moving_prices.step(prices)\n",
    "        moving_volumes = self.moving_volumes.step(volumes)\n",
    "\n",
    "        if len(self.price_history) < self.window_size or not self.moving_prices.ready:\n",
    "            return Gaussian(pd.Series([]),[])\n",
    "\n",
    "        if self.prev_fair is None:\n",
    "            self.prev_fair = Gaussian(moving_prices, [1e100 for _ in df.columns])\n",
    "            \n",
    "        if self.coint_f is None:\n",
    "            self.coint_f = pd.DataFrame(1, index=df.columns, columns=df.columns)\n",
    "            \n",
    "        A = np.vstack([df.index.values, np.ones(len(df.index))]).T\n",
    "        Q = np.linalg.lstsq(A, df, rcond=None)[0]\n",
    "        trend = A @ Q\n",
    "        df -= trend\n",
    "#         assert(0)\n",
    "\n",
    "        # calculate p values for pair cointegration\n",
    "        if self.sample_counter == 0:\n",
    "            slow = df.rolling(256, center=True).mean()[128:-128]\n",
    "            df2 = df.iloc[128:-128] - slow\n",
    "            for i in df.columns:\n",
    "                for j in df.columns:\n",
    "                    if i >= j:\n",
    "                        continue\n",
    "                    p = coint(df2[i], df2[j], trend='ct', maxlag=self.maxlag, autolag=None)[1]\n",
    "                    f = 1 + p * p * p * 15625 # .04 -> 2, .05 -> ~3, .1 -> ~16\n",
    "                    self.coint_f.loc[i,j] = f\n",
    "                    self.coint_f.loc[j,i] = f\n",
    "#             print(self.coint_f)\n",
    "        self.sample_counter = (self.sample_counter - 1) % self.cointegration_period\n",
    "        \n",
    "        diffs = df.diff()[1:]\n",
    "        var = df.var()\n",
    "        stddev = np.sqrt(var) + 1e-100\n",
    "        r = df.corr()\n",
    "        r2 = r * r\n",
    "#         print(r2)\n",
    "#         (df / df.std()).plot()\n",
    "#         assert(0)\n",
    "        correlated_slopes = r.mul(stddev, axis=1).div(stddev, axis=0)\n",
    "        price_ratios = moving_prices[np.newaxis, :] / moving_prices[:, np.newaxis]\n",
    "        delta = prices - moving_prices\n",
    "        volume_signals = np.sqrt(moving_volumes * moving_prices)\n",
    "        volume_f = np.max(volume_signals) / volume_signals\n",
    "        fair_delta_means = correlated_slopes.mul(delta, axis=0)\n",
    "        delta_vars = diffs.rolling(self.movement_half_life).sum()[self.movement_half_life:].var()\n",
    "        correlated_delta_vars = delta_vars[:, np.newaxis] * np.square(price_ratios)\n",
    "        fair_delta_vars = volume_f * self.coint_f * ((1-r2) * delta_vars[np.newaxis, :] + r2 * correlated_delta_vars)\n",
    "        fair_deltas = [Gaussian(fair_delta_means.loc[i], fair_delta_vars.loc[i]) for i in df.columns]\n",
    "        fair_delta = Gaussian.intersect(fair_deltas)\n",
    "        disagreement = pd.DataFrame([bhattacharyya_multi(fair_delta, f) for f in fair_deltas])\n",
    "        fair_delta = Gaussian(fair_delta.mean, fair_delta.variance * (1 + disagreement.pow(2).sum()))\n",
    "        absolute_fair = fair_delta + moving_prices\n",
    "\n",
    "        step = prices - (self.prev_fair.mean + self.moving_prices.trend)\n",
    "        fair_step_means = correlated_slopes.mul(step, axis=0)\n",
    "        step_vars = diffs.var()\n",
    "        correlated_step_vars = step_vars[:, np.newaxis] * np.square(price_ratios)\n",
    "        fair_step_vars = volume_f * self.coint_f * ((1-r2) * step_vars[np.newaxis, :] + r2 * correlated_step_vars)\n",
    "        fair_steps = [Gaussian(fair_step_means.loc[i], fair_step_vars.loc[i]) for i in df.columns]\n",
    "        fair_step =  Gaussian.intersect(fair_steps)\n",
    "        disagreement_step = pd.DataFrame([bhattacharyya_multi(fair_step, f) for f in fair_steps])\n",
    "        fair_step = Gaussian(fair_step.mean, fair_step.variance * (1 + disagreement_step.pow(2).sum()))\n",
    "        relative_fair = fair_step + self.prev_fair + self.moving_prices.trend\n",
    "        \n",
    "        fair = absolute_fair & relative_fair\n",
    "#         print(\"\\n\\nASDSA\")\n",
    "#         print(prices)\n",
    "#         print(absolute_fair)\n",
    "#         print(relative_fair)\n",
    "#         print(fair)\n",
    "        self.prev_fair = fair\n",
    "        return fair[frame.index]\n",
    "\n",
    "analyze(run(KalmanFilter2(window_size = 500, movement_half_life = 6, trend_half_life = 256, cointegration_period=32, maxlag = 8), data_15min, fees=0.00075, min_edge=0.0005, min_edge_to_close=0.0002\n",
    "           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "import pickle\n",
    "\n",
    "from research.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, Gaussian\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from trader.util.linalg import orthogonal_projection, hyperplane_projection\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def johansen(P, maxlag):\n",
    "    mean = P.mean()\n",
    "    c = coint_johansen(P / mean - 1, det_order=-1, k_ar_diff=maxlag)\n",
    "    significant_results = (c.lr1 > c.cvt[:,1]) * (c.lr2 < c.cvm[:,2])\n",
    "    Q = pd.DataFrame(c.evec[:,significant_results].T, columns=P.columns)\n",
    "    return mean, Q.div(Q.apply(np.linalg.norm, axis=1), axis=0)\n",
    "\n",
    "# TODO: reimplement with vector autoregression test, similar to johansen internals.\n",
    "def test_coint(P, mean_train, q, maxlag):\n",
    "    x = (P / mean_train - 1) @ q\n",
    "    return adfuller(x, regression=\"nc\", maxlag=maxlag, autolag=None)[1]\n",
    "\n",
    "def cosine_similar_to_any(Q, x):\n",
    "    for q in Q:\n",
    "        if distance.cosine(q, x) < 0.1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class LiveCointegrator(Strategy):\n",
    "    def __init__(self, train_size, validation_size, cointegration_period, maxlag):\n",
    "        self.window_size = train_size + validation_size\n",
    "        self.train_size = train_size\n",
    "        self.validation_size = validation_size\n",
    "        self.cointegration_period = cointegration_period\n",
    "        self.maxlag = maxlag\n",
    "        self.sample_counter = 0\n",
    "        self.price_history = None\n",
    "        self.Q = None\n",
    "        self.base_prices = None\n",
    "#         self.covs = None\n",
    "        self.base_cov = None\n",
    "        self.prev_fair = None\n",
    "        self.coints = []\n",
    "\n",
    "    def step(self, frame):\n",
    "        prices = frame[\"price\"]\n",
    "        \n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(self.window_size, dtype=(np.float64, len(prices.index)))\n",
    "            \n",
    "        if self.prev_fair is None:\n",
    "            self.prev_fair = self.null_estimate(prices)\n",
    "            \n",
    "        self.price_history.append(prices)\n",
    "        \n",
    "        if len(self.price_history) < self.window_size:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        if self.sample_counter == 0:\n",
    "            P = pd.DataFrame(self.price_history, columns=prices.index)\n",
    "            P_train = P.iloc[:self.train_size]\n",
    "            P_val = P.iloc[self.train_size:]\n",
    "            mean, candidates = johansen(P_train, self.maxlag)\n",
    "            self.base_prices = mean\n",
    "            new_coints = [q for q in candidates.values if test_coint(P_val, mean, q, self.maxlag) < .05]\n",
    "            old_coints = [q for q in self.coints if test_coint(P_val, mean, q, self.maxlag) < .05 and not cosine_similar_to_any(new_coints, q)]\n",
    "            self.coints = new_coints + old_coints\n",
    "            self.base_cov = (P / P.mean()).cov()\n",
    "            \n",
    "        self.sample_counter -= 1\n",
    "        self.sample_counter %= self.cointegration_period\n",
    "        \n",
    "        if not self.coints:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        fair_means = [hyperplane_projection(prices / self.base_prices - 1, q) for q in self.coints]\n",
    "        fair = Gaussian.intersect([(Gaussian(mean, self.base_cov) + 1) * self.base_prices for mean in fair_means])\n",
    "        \n",
    "#         step_fair_means = [hyperplane_projection(prices / self.prev_fair.mean - 1, q) for q in self.coints]\n",
    "#         step_fair = Gaussian.intersect([Gaussian(mean, self.base_cov) * self.prev_fair.mean for mean in step_fair_means])\n",
    "        \n",
    "#         fair = fair & (self.prev_fair + step_fair)\n",
    "        self.prev_fair = fair\n",
    "        return fair\n",
    "\n",
    "analyze(run(LiveCointegrator(train_size=500, validation_size = 100, cointegration_period=64, maxlag=8), data_15min, fees=0.001, min_edge=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "import pickle\n",
    "\n",
    "from research.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, Gaussian\n",
    "from trader.util.linalg import orthogonal_projection, hyperplane_projection\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "def plot_gaussian(x, y, gaussian):\n",
    "    z = gaussian.pdf(np.dstack(np.meshgrid(x,y)))\n",
    "    plt.pcolormesh(x, y, z)\n",
    "    plt.show()\n",
    "\n",
    "class LivePairCointegrator(Strategy):\n",
    "    def __init__(self, window_size, cointegration_frequency=4):\n",
    "        self.window_size = window_size\n",
    "        self.cointegration_period = window_size // cointegration_frequency\n",
    "        self.sample_counter = 0\n",
    "        self.price_history = None\n",
    "        self.prediction_covs = {}\n",
    "\n",
    "    def step(self, frame):\n",
    "        prices = frame[\"price\"]\n",
    "        \n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(self.window_size, dtype=(np.float64, len(prices.index)))\n",
    "            \n",
    "        self.price_history.append(prices)\n",
    "        \n",
    "        if len(self.price_history) < self.window_size:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        df = pd.DataFrame(self.price_history, columns=prices.index)\n",
    "        self.mean = df.mean()\n",
    "        deltas = df / self.mean - 1\n",
    "        stddev = deltas.std() + 1e-100\n",
    "        regression_slope = deltas.corr().mul(stddev, axis=1).div(stddev, axis=0)\n",
    "        cov = deltas.cov()\n",
    "        \n",
    "        if self.sample_counter == 0:\n",
    "            \n",
    "            for pair_i in prices.index:\n",
    "                for pair_j in prices.index:\n",
    "                    if pair_i >= pair_j:\n",
    "                        continue\n",
    "                    deltas_ij = deltas[[pair_i, pair_j]]\n",
    "                    regression_vector = [regression_slope.loc[pair_i][pair_j], -1]\n",
    "                    residuals = orthogonal_projection(deltas_ij, regression_vector)\n",
    "                    cov_resid = residuals.cov()\n",
    "                    cov_pred = (deltas_ij - residuals).cov()\n",
    "                    print(cov_pred)\n",
    "                    # We can calculate the cointegration p-value more efficiently by reusing\n",
    "                    # the residuals found via the orthogonal projection but getting it right is\n",
    "                    # tricky. So we just do it the dumb way\n",
    "                    p = coint(deltas[pair_j], deltas[pair_i], trend='nc', maxlag=0, autolag=None)[1]\n",
    "#                     # regularize prediction covariance by using the raw price covariance shape with the \n",
    "#                     # prediction covariance slope\n",
    "#                     cov_sam = deltas_ij.cov()\n",
    "#                     w_pred, v_pred = np.linalg.eigh(cov_pred)\n",
    "#                     w_sam, v_sam = np.linalg.eigh(cov_sam)\n",
    "#                     eigenvalue_ratio_sam = w_sam[1] / w_sam[0]\n",
    "#                     w_reg = w_pred[1] * np.array([eigenvalue_ratio_sam, 1])\n",
    "#                     cov_reg = v_pred @ np.diag(w_reg) @ v_pred.T\n",
    "#                     # discount predictions based on p-value\n",
    "#                     cov_reg *= max(1, p * p * 900) # min(1e4, np.cosh(16 * p))\n",
    "#                     cov_reg = pd.DataFrame(cov_reg, index=deltas_ij.columns, columns=deltas_ij.columns)\n",
    "                    cov_reg = cov_pred + max(1, p * p * 900) * cov_resid\n",
    "                    self.prediction_covs[pair_i, pair_j] = cov_reg\n",
    "                    self.prediction_covs[pair_j, pair_i] = cov_reg.loc[::-1, ::-1]\n",
    "                    \n",
    "                    # uncomment and run this if you want to see what the prediction covariances look like\n",
    "                    \n",
    "                    prediction = Gaussian(pd.Series(0, index=deltas_ij.columns), cov_resid)\n",
    "                    base = Gaussian(pd.Series(0, index=deltas_ij.columns),  cov_pred)\n",
    "                    regularized = Gaussian(pd.Series(0, index=deltas_ij.columns), cov_reg)\n",
    "                    xrange = deltas[pair_i].max() - deltas[pair_i].min()\n",
    "                    yrange = deltas[pair_j].max() - deltas[pair_j].min()\n",
    "                    x = np.linspace(-xrange, xrange)\n",
    "                    y = np.linspace(-yrange, yrange)\n",
    "                    plot_gaussian(x, y, prediction)\n",
    "                    plot_gaussian(x, y, base)\n",
    "                    plot_gaussian(x, y, regularized)\n",
    "            \n",
    "        self.sample_counter -= 1\n",
    "        self.sample_counter %= self.cointegration_period\n",
    "        \n",
    "        fairs = []\n",
    "        delta = prices / self.mean - 1\n",
    "        cov = deltas.cov()\n",
    "        for pair_i in prices.index:\n",
    "            for pair_j in prices.index:\n",
    "                if pair_i >= pair_j:\n",
    "                    continue\n",
    "                regression_vector = [regression_slope.loc[pair_i][pair_j], -1]\n",
    "#                 residuals = orthogonal_projection(deltas[[pair_i, pair_j]], regression_vector)\n",
    "                fair_delta_mean = hyperplane_projection(delta[[pair_i, pair_j]], regression_vector)\n",
    "                fair_cov = self.prediction_covs[pair_i, pair_j]\n",
    "                fair = Gaussian(fair_delta_mean, fair_cov)\n",
    "#                 fair = Gaussian(fair_delta_mean, cov.loc[[pair_i, pair_j], [pair_i, pair_j]])\n",
    "#                 scale = fair.z_score(delta[[pair_i, pair_j]])\n",
    "#                 fair = Gaussian(fair.mean, fair.covariance * scale)\n",
    "#                 print(prices)\n",
    "#                 print(fair)\n",
    "#                 x = np.linspace(deltas[pair_i].min(), deltas[pair_i].max()) - deltas[pair_i].mean()\n",
    "#                 y = np.linspace(deltas[pair_j].min(), deltas[pair_j].max()) - deltas[pair_j].mean()\n",
    "#                 plot_gaussian(x, y, fair)\n",
    "#                 plot_gaussian(x, y, Gaussian(pd.Series(0, index=[pair_i, pair_j]), residuals.cov()))\n",
    "#                 plot_gaussian(x, y, Gaussian(pd.Series(0, index=[pair_i, pair_j]), deltas[[pair_i, pair_j]].cov()))\n",
    "                fairs.append(fair)\n",
    "#         print(fairs)\n",
    "        fair = (Gaussian.intersect(fairs) + 1) * self.mean\n",
    "#         print(prices)\n",
    "#         print(fair)\n",
    "#         x = np.linspace(df['BTC_USDT'].min(), df['BTC_USDT'].max())\n",
    "#         y = np.linspace(df['ETH_USDT'].min(), df['ETH_USDT'].max())\n",
    "#         plot_gaussian(x, y, fair[['BTC_USDT', 'ETH_USDT']])\n",
    "#         print(fair / prices - 1)\n",
    "        return fair\n",
    "\n",
    "analyze(run(LivePairCointegrator(window_size=96), data_15min.tail(1500), fees=0.001, min_edge=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CointegratorStrategy(cointegration_window_size = 16), data_15min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(KalmanFilterStrategy(\n",
    "    correlation_window_size = 480,\n",
    "    movement_half_life = 1\n",
    "), tail_data(data_min, 10000), fees = 0.002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategy import CombinedStrategy\n",
    "\n",
    "analyze(run(CombinedStrategy([\n",
    "    KalmanFilterStrategy(correlation_window_size = 60, movement_half_life = 3),\n",
    "    CointegratorStrategy(cointegration_window_size = 16)\n",
    "]), data_15min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CointegratorStrategy(cointegration_window_size = 512), data_5min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(KalmanFilterStrategy(correlation_window_size = 165, movement_half_life = 70), data_5min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CombinedStrategy([\n",
    "    KalmanFilterStrategy(correlation_window_size = 16, movement_half_life = 8),\n",
    "    CointegratorStrategy(cointegration_window_size = 64)\n",
    "]), tail_data(data_min, 1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def find_best_window_sizes(data, n):\n",
    "    points = []\n",
    "    best = None\n",
    "    best_ror = 0\n",
    "    for _ in range(n):\n",
    "        movement_half_life = random.expovariate(1) * 15\n",
    "#         window_ratio = random.uniform(1, 10)\n",
    "#         window_size = max(3, int(movement_half_life * window_ratio))\n",
    "#         movement_half_life = 2\n",
    "        window_size = int(random.expovariate(1) * 100) + 400\n",
    "#         window_size = int(random.expovariate(1) * 30) + 3\n",
    "#         window_size = 4\n",
    "#         window_size = 32\n",
    "        print('Trying window_size: {0} and half_life: {1}'.format(window_size, movement_half_life))\n",
    "        ror = analyze(run(KalmanFilter(window_size, movement_half_life), data, fees = 0.002), plot=False)\n",
    "        print('  RoR: {0}'.format(ror))\n",
    "        point = { 'window_size': window_size, 'half_life': movement_half_life, 'RoR': ror }\n",
    "        points.append(point)\n",
    "        if ror > best_ror:\n",
    "            best = point\n",
    "            best_ror = ror\n",
    "    print('Best found:')\n",
    "    print(best)\n",
    "    pd.DataFrame(points).plot.scatter('window_size', 'half_life', c='RoR', colormap='jet')\n",
    "    \n",
    "find_best_window_sizes(tail_data(data_min, 1500), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
