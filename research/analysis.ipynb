{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy scipy pandas matplotlib numpy_ringbuffer sklearn\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currencies = ['usd', 'btc', 'eth', 'ltc', 'xrp', 'eos']\n",
    "# pairs = [c + '_usd' for c in currencies if c != 'usd']\n",
    "# volume_keys = [c + '_tx_volume' for c in currencies if c != 'usd']\n",
    "\n",
    "# def prep_data(file):\n",
    "#     data = pickle.load(open(file, 'rb'))\n",
    "#     dates = [x['date'] for x in data]\n",
    "#     prices = [{k:v for k,v in x.items() if k in pairs} for x in data]\n",
    "#     volumes = [{(k.partition('_')[0] + '_usd'):v for k,v in x.items() if k in volume_keys} for x in data]\n",
    "#     return {\n",
    "#         'prices': pd.DataFrame(prices, index = dates),\n",
    "#         'volumes': pd.DataFrame(volumes, index = dates)\n",
    "#     }\n",
    "\n",
    "# def reduce_data(data, resampling):\n",
    "#     '''Averages prices, sums volumes'''\n",
    "#     prices = data['prices'].resample(resampling).first().fillna(method='ffill')\n",
    "#     volumes = data['volumes'].resample(resampling).sum().fillna(method='ffill')\n",
    "#     return { 'prices': prices, 'volumes': volumes }\n",
    "\n",
    "# def tail_data(data, n):\n",
    "#     '''get the last n points of the given data'''\n",
    "#     prices = data['prices'].tail(n)\n",
    "#     volumes = data['volumes'].tail(n)\n",
    "#     return { 'prices': prices, 'volumes': volumes }\n",
    "\n",
    "# def viz_data(data):\n",
    "#     '''Only plots prices for now'''\n",
    "#     plt.plot(data['prices'] / data['prices'].mean() - 1)\n",
    "#     plt.show()\n",
    "\n",
    "# def find_gaps(data, freq):\n",
    "#     idx_ref = pd.date_range(start=data.index[0], end=data.index[-1],freq=freq)\n",
    "#     gaps = idx_ref[~idx_ref.isin(data.index)]\n",
    "#     return gaps\n",
    "\n",
    "# data = prep_data('data/data.p')\n",
    "# data_min = reduce_data(prep_data('data/data-minute.p'), '1Min')\n",
    "# data_5min = reduce_data(data_min, '5Min')\n",
    "# data_15min = reduce_data(data_min, '15Min')\n",
    "# viz_data(data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min = pd.read_hdf('data/1min.h5')\n",
    "data_15min = data_min.resample('15Min').first()\n",
    "data_1h = data_min.resample('1h').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.util.stats import Ema, HoltEma\n",
    "price_data = data_15min.xs('price', axis=1, level=1)\n",
    "h = HoltEma(64, 512)\n",
    "smooth = price_data.apply(h.step, axis=1).rename(columns=lambda x: str(x) + '_smooth')\n",
    "together = pd.concat([price_data, smooth], axis=1)\n",
    "(together/together.mean()).plot(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "h = HoltEma(3, 3)\n",
    "def htrend(d):\n",
    "    h.step(d)\n",
    "    return h.trend\n",
    "price_data_ = price_data[-300:].filter(regex=\"-LTC-\")\n",
    "trend = price_data_.apply(htrend, axis=1).rename(columns=lambda x: str(x) + '_trend')\n",
    "e = Ema(9)\n",
    "ema_trend = price_data_.diff()[1:].apply(e.step, axis=1).rename(columns=lambda x: str(x) + '_ema_trend')\n",
    "h2 = HoltEma(9, 9)\n",
    "hema_trend = price_data_.diff()[1:].apply(h2.step, axis=1).rename(columns=lambda x: str(x) + '_hema_trend')\n",
    "(price_data_ / price_data_.mean()).plot(figsize=(16,10))\n",
    "together = pd.concat([trend, ema_trend, hema_trend], axis=1)\n",
    "((together - together.mean()) / together.std()).plot(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.util.types import TradingPair, ExchangePair\n",
    "from trader.util.constants import LTC, USDT, BINANCE\n",
    "price_data_ = price_data[-100:].filter(regex=\"-LTC-\")\n",
    "diffs = price_data_.diff()[1:]\n",
    "diffs /= diffs.std()\n",
    "x = diffs.abs().sort_values(by=ExchangePair(BINANCE, TradingPair(LTC, USDT))).reset_index(drop=True)\n",
    "diffs.plot()\n",
    "x.plot()\n",
    "price_data_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_matrix(df,size=10):\n",
    "#     '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "#     Input:\n",
    "#         df: pandas DataFrame\n",
    "#         size: vertical and horizontal size of the plot'''\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(size, size))\n",
    "#     ax.matshow(df)\n",
    "#     plt.xticks(range(len(df.columns)), df.columns);\n",
    "#     plt.yticks(range(len(df.index)), df.index);\n",
    "#     # Loop over data dimensions and create text annotations.\n",
    "#     for i in range(len(df.index)):\n",
    "#         for j in range(len(df.columns)):\n",
    "#             ax.text(j, i, '{:0.2f}'.format(df.iloc[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "# plot_matrix(data_15min['prices'].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_correlate_(x, y):\n",
    "#     return np.argmax(np.correlate(x, y, mode='full')) - len(x) + 1\n",
    "\n",
    "# def cross_correlate(df):\n",
    "#     '''Compute cross-correlation matrix for the given dataframe.'''\n",
    "#     ccs = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "#     for i in df.columns:\n",
    "#         for j in df.columns:\n",
    "#             if i == j:\n",
    "#                 ccs.loc[i,j] = 0\n",
    "#                 continue\n",
    "#             if np.isnan(ccs.loc[i,j]):\n",
    "#                 ccs.loc[i,j] = cross_correlate_(df[i], df[j])\n",
    "#                 ccs.loc[j,i] = -ccs.loc[i,j]\n",
    "#     return ccs\n",
    "\n",
    "# print(cross_correlate(pd.DataFrame([[1,2],[2,1],[1,2],[2,1],[1,2]])))\n",
    "# print(cross_correlate(pd.DataFrame([[1,1],[2,2],[3,3],[4,4],[5,5]])))\n",
    "    \n",
    "# print(cross_correlate(data_min['prices']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from trader.util.constants import BCH, BTC, EOS, ETH, LTC, NEO, XRP\n",
    "from trader.util.stats import Ema\n",
    "\n",
    "# TODO: fetch this dynamically\n",
    "CIRCULATING_SUPPLY = pd.Series(\n",
    "    {BTC: 18e6, ETH: 106e6, XRP: 42e9, BCH: 18e6, EOS: 913e6, LTC: 62e6, NEO: 65e6}\n",
    ")\n",
    "\n",
    "# TODO: actually convert quotes, not just volume\n",
    "# think about how to do this. what if a quote_usd pair does not exist on a :particular exchange?\n",
    "def convert_quotes_to_usd(frame):\n",
    "    frame = frame.copy()\n",
    "    # .values call necessary because assigning to indexslices is buggy\n",
    "    # see https://github.com/pandas-dev/pandas/issues/10440\n",
    "    frame.loc[pd.IndexSlice[:, \"volume\"]] = (frame.xs(\"volume\", level=1) * frame.xs(\"price\", level=1)).values\n",
    "    return frame\n",
    "\n",
    "\n",
    "def aggregate_currency_quotes(moving_volumes, frame):\n",
    "    \"\"\"\n",
    "    Aggregate quotes for the same base currency across all exchanges and pairs. Quotes should\n",
    "    already be in USD.\n",
    "\n",
    "    Returns aggregate quotes.\n",
    "\n",
    "    Note: this may cause unwanted fluctuation in the aggregate price if quotes disagree - and\n",
    "    trading volume moves back and forth between them. To combat this we weight quotes by a slow\n",
    "    moving average of trading volume, but is there a better solution?\n",
    "    \"\"\"\n",
    "    currencies = {pair.base for pair in frame.index.unique(level=0)}\n",
    "    index = pd.MultiIndex.from_product([currencies, frame.index.unique(level=1)])\n",
    "    aggregates = pd.Series(index=index)\n",
    "    for c in currencies:\n",
    "        components = frame.filter(regex=\"-\" + str(c) + \"-.*\", axis=0)\n",
    "        moving_volumes_c = moving_volumes.filter(regex=\"-\" + str(c) + \"-.*\") + 1e-10\n",
    "        volume = components.xs(\"volume\", level=1).sum()\n",
    "        price = components.xs(\"price\", level=1) @ moving_volumes_c / moving_volumes_c.sum()\n",
    "        aggregates.loc[pd.IndexSlice[c, :]] = (price, volume)\n",
    "    \n",
    "    return aggregates\n",
    "\n",
    "\n",
    "def compute_baskets(basket_specs, aggregates):\n",
    "    \"\"\"\n",
    "    Compute a cap-weighted basket of currencies. Input should have aggregate quotes.\n",
    "    \"\"\"\n",
    "    index = pd.MultiIndex.from_product([basket_specs, aggregates.index.unique(level=1)])\n",
    "    baskets = pd.Series(index=index)\n",
    "    for name, currencies in basket_specs.items():\n",
    "        components = aggregates.loc[pd.IndexSlice[currencies, :]]\n",
    "        # scale down to avoid numerical instability\n",
    "        price = components.xs(\"price\", level=1) @ CIRCULATING_SUPPLY[currencies] / 1e9\n",
    "        volume = components.xs(\"volume\", level=1).sum()\n",
    "        baskets.loc[pd.IndexSlice[name, :]] = (price, volume)\n",
    "    return baskets\n",
    "\n",
    "class SignalAggregator:\n",
    "    \"\"\"\n",
    "    Adds cap-weighted baskets to frame.\n",
    "    TODO: also convert non-USD quotes to USD, add aggregated currency prices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, volume_half_life, baskets):\n",
    "        self.__moving_volumes = Ema(volume_half_life)\n",
    "        self.__baskets = baskets\n",
    "\n",
    "    def step(self, frame):\n",
    "        frame = convert_quotes_to_usd(frame)\n",
    "        moving_volumes = self.__moving_volumes.step(frame.xs(\"volume\", level=1))\n",
    "        aggregates = aggregate_currency_quotes(moving_volumes, frame)\n",
    "        baskets = compute_baskets(self.__baskets, aggregates)\n",
    "        signals = pd.concat([aggregates, baskets]).astype(np.float64)\n",
    "        return signals, frame\n",
    "        \n",
    "# TODO: vectorize this\n",
    "def prep_warmup_data(data, signal_aggregator, warmup_len):\n",
    "    warmup = data.iloc[:warmup_len]\n",
    "    data = data.iloc[warmup_len:]\n",
    "    signal_frames, data_frames = zip(*[signal_aggregator.step(frame) for _, frame in warmup.iterrows()])\n",
    "    warmup_signals = pd.DataFrame(signal_frames, index=warmup.index)\n",
    "    warmup_data = pd.DataFrame(data_frames, index=warmup.index)\n",
    "    return (warmup_data, warmup_signals), data\n",
    "    \n",
    "signal_aggregator_15min = SignalAggregator(500, { \"total_market\": [BTC, ETH, XRP, LTC, EOS, NEO] })\n",
    "(warmup_data_15min, warmup_signals_15min), data_15min_ = prep_warmup_data(data_15min, signal_aggregator_15min, 500)\n",
    "print(warmup_signals_15min.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trader.util.stats import HoltEma, Emse, TrendEstimator\n",
    "from trader.util.maths import sigmoid\n",
    "\n",
    "class ExecutionStrategy:\n",
    "    def __init__(self, size, trend_hl, accel_hl, fair_trend_hl, fair_accel_hl, warmup_data):\n",
    "        self.size = size\n",
    "        # Does it make sense to replace with something else? (like % of takes that are buys)\n",
    "        \n",
    "        warmup_prices = warmup_data.xs(\"price\", axis=1, level=1)\n",
    "        most_recent_prices = warmup_prices.iloc[-1]\n",
    "        self.trend_estimator = TrendEstimator(HoltEma(trend_hl, accel_hl), most_recent_prices)\n",
    "        self.fair_trend_estimator = TrendEstimator(HoltEma(fair_trend_hl, fair_accel_hl), most_recent_prices)\n",
    "        for _, prices in warmup_prices.iloc[-4 * fair_accel_hl:].iterrows():\n",
    "            self.trend_estimator.step(prices)\n",
    "            self.fair_trend_estimator.step(prices) # should this be different?\n",
    "\n",
    "    def tick(self, positions, prices, fairs, fees):\n",
    "        \"\"\"Takes fair as Gaussian, positions in base currency.\n",
    "        Returns orders in base currency (negative size indicates sell).\n",
    "\n",
    "        Since our fair estimate may have skewed uncertainty, it may be the case that\n",
    "        a price change for one trading pair causes us to desire positions in other\n",
    "        pairs. Therefore get_orders needs to consider the entire set of fairs and\n",
    "        bid/asks at once.\n",
    "\n",
    "        TODO: generalize to multiple quotes\n",
    "        TODO: use books instead of just the best bid/ask price\n",
    "        \"\"\"\n",
    "        # bookkeepping\n",
    "        trend = self.trend_estimator.step(prices)\n",
    "        fair_trend = self.fair_trend_estimator.step(fairs.mean)\n",
    "        \n",
    "        pct_edge = fairs.mean / prices - 1\n",
    "        net_pct_edge = np.sign(pct_edge) * np.maximum(0, np.abs(pct_edge) - 2 * fees)\n",
    "        z_edge = (fairs.mean - prices) / fairs.stddev\n",
    "        target_position_values = np.sign(pct_edge) * np.sqrt(z_edge * net_pct_edge * 100) * self.size\n",
    "        pair_positions = positions[[(ep.exchange_id, ep.base) for ep in prices.index]].set_axis(\n",
    "            prices.index, inplace=False\n",
    "        )\n",
    "        proposed_orders = target_position_values / fairs.mean - pair_positions\n",
    "        profitable = np.sign(proposed_orders) * pct_edge > 2 * fees\n",
    "        trending_correctly = (trend * np.sign(pct_edge) > 0) & (fair_trend * np.sign(pct_edge) > 0)\n",
    "        profitable_orders = proposed_orders * profitable * trending_correctly\n",
    "\n",
    "        unprofitable_position = np.sign(pair_positions) * pct_edge < 0\n",
    "        position_closing_orders = -pair_positions * (profitable_orders == 0) * unprofitable_position * trending_correctly\n",
    "\n",
    "        return profitable_orders + position_closing_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import numpy as np\n",
    "import pandas as pd\n",
    "from trader.util import Gaussian\n",
    "from abc import ABC, abstractmethod\n",
    "from trader.util.stats import HoltEma\n",
    "\n",
    "# Note: Assumes all orders fill at last trade price. Attempting to simulate market-making would\n",
    "# require combing through book and trade data, which is too much work for us to do at the moment.\n",
    "\n",
    "\n",
    "def execute_orders(fees, prices, balances, orders):\n",
    "    for (ep, size) in orders.items():\n",
    "        value = size * prices[ep]\n",
    "        balances[(ep.exchange_id, ep.quote)] -= value\n",
    "        balances[(ep.exchange_id, ep.quote)] -= abs(value) * fees\n",
    "        balances[(ep.exchange_id, ep.base)] += size\n",
    "\n",
    "\n",
    "def run(\n",
    "    strategy,\n",
    "    execution_strategy,\n",
    "    data,\n",
    "    aggregator = SignalAggregator(1,{}),\n",
    "    fees=0\n",
    "):\n",
    "    eps = data.columns.unique(0)\n",
    "    balances = pd.Series(0.0, index=[(ep.exchange_id, ep.base) for ep in eps])\n",
    "    quote_balances = pd.Series(0.0, index={(ep.exchange_id, ep.quote) for ep in eps})\n",
    "    balances = balances.append(quote_balances)\n",
    "    balance_history = []\n",
    "    fair_history = []\n",
    "    print(\"running\")\n",
    "    for _, frame in data.iterrows():\n",
    "        signals, frame = aggregator.step(frame)\n",
    "#         print(signals)\n",
    "        fairs = strategy.tick(frame, signals)\n",
    "        fairs &= Gaussian(frame.xs('price',level=1), [1e100 for _ in eps])\n",
    "        \n",
    "        orders = execution_strategy.tick(balances, frame.xs('price',level=1), fairs, fees)\n",
    "        execute_orders(fees, frame.xs('price',level=1), balances, orders)\n",
    "\n",
    "        fair_history.append(fairs)\n",
    "        balance_history.append(balances.copy())\n",
    "    return {\n",
    "        'data': data,\n",
    "        'fairs': pd.DataFrame(fair_history, index=data.index),\n",
    "        'balances': pd.DataFrame(balance_history, index=data.index)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# number of ticks to sum for price movements in risk calculation\n",
    "RISK_WINDOW = 10\n",
    "\n",
    "\n",
    "def principal_market_movements(prices):\n",
    "    \"\"\"Returns principal vectors for 1-stddev market movements, plus explained variance ratios\"\"\"\n",
    "    # Fit PCA to scaled (mean 0, variance 1) matrix of single-tick price differences\n",
    "    pca = PCA(n_components=0.97)\n",
    "    scaler = StandardScaler()\n",
    "    price_deltas = prices.diff().iloc[1:].rolling(RISK_WINDOW).sum().iloc[RISK_WINDOW:]\n",
    "    price_deltas_scaled = scaler.fit_transform(price_deltas)\n",
    "    pca.fit(price_deltas_scaled)\n",
    "    pcs = pd.DataFrame(scaler.inverse_transform(pca.components_), columns=price_deltas.columns)\n",
    "    return (pcs, pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "def max_abs_drawdown(pnls):\n",
    "    \"\"\"Maximum peak-to-trough distance before a new peak is attained. The usual metric, expressed\n",
    "    as a fraction of peak value, does not make sense in the infinite-leverage context.\"\"\"\n",
    "    max_drawdown = 0\n",
    "    peak = -np.inf\n",
    "    for pnl in pnls:\n",
    "        if pnl > peak:\n",
    "            peak = pnl\n",
    "        drawdown = peak - pnl\n",
    "        if drawdown > max_drawdown:\n",
    "            max_drawdown = drawdown\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def analyze(results, plot=True):\n",
    "    \"\"\"Analyzes P/L and various risk metrics for the given run results.\n",
    "    Plots balances (with P/L) and market risk over time.\n",
    "\n",
    "    Note: RoRs are per-tick. They are NOT comparable across time scales.\"\"\"\n",
    "    # Balance values\n",
    "    price_data = results[\"data\"].xs(\"price\", axis=1, level=1)\n",
    "    quote_currency = (price_data.columns[0].exchange_id, price_data.columns[0].quote)\n",
    "    prices_ = price_data.rename(columns=lambda pair: (pair.exchange_id, pair.base))\n",
    "    prices_[quote_currency] = 1\n",
    "    balance_values = results[\"balances\"] * prices_\n",
    "\n",
    "    pnls = balance_values.sum(axis=1)\n",
    "    pnl = pnls.iloc[-1]\n",
    "\n",
    "    # Market risk\n",
    "    (pmms, pmm_weights) = principal_market_movements(price_data)\n",
    "    balances_ = results[\"balances\"][\n",
    "        [(pair.exchange_id, pair.base) for pair in price_data.columns]\n",
    "    ].set_axis(price_data.columns, axis=1, inplace=False)\n",
    "    component_risks = np.abs(balances_ @ pmms.T)\n",
    "    risks = component_risks @ pmm_weights\n",
    "\n",
    "    total_positions = np.abs(balance_values.drop(columns=[quote_currency]).values).sum()\n",
    "    max_drawdown = max_abs_drawdown(pnls)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(figsize=(16, 4))\n",
    "        balance_values.plot(ax=axs)\n",
    "        pd.DataFrame(pnls, columns=[\"P/L\"]).plot(ax=axs)\n",
    "        axs.axhline(0, color=\"grey\")\n",
    "        plt.show()\n",
    "        print(f\"Return on maximum market risk: {pnl / (risks.values.max() + 1e-10)}\")\n",
    "        print(f\"Return on maximum drawdown:    {pnl / (max_drawdown + 1e-10)}\")\n",
    "        print(f\"Return on total market risk:   {pnl / (risks.values.sum() + 1e-10)}\")\n",
    "        print(f\"Return on total positions:     {pnl / (total_positions + 1e-10)}\")\n",
    "        print(f\"Sharpe ratio:                  {pnl / (pnls.std() + 1e-10)}\")\n",
    "        print(f\"Final P/L:                     {pnl}\")\n",
    "        print(f\"Maximum market risk:           {risks.values.max()}\")\n",
    "        print(f\"Maximum drawdown:              {max_drawdown}\")\n",
    "        print(f\"Final balances:\")\n",
    "        print(results[\"balances\"].iloc[-1])\n",
    "\n",
    "    return pnl / (risks.values.max() + 1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trader.strategy import Strategy\n",
    "from trader.util import Gaussian\n",
    "\n",
    "class DummyStrategy(Strategy):\n",
    "    \"\"\"A strategy that always returns a null prediction.\"\"\"\n",
    "\n",
    "    def tick(self, frame, signals):\n",
    "        return Gaussian(pd.Series([]), [])\n",
    "\n",
    "\n",
    "execution_strategy = ExecutionStrategy(1000, 500, 1, 3, 2, 6, warmup_data_15min)\n",
    "analyze(run(DummyStrategy(), execution_strategy, data_min.tail(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_aggregator_min = SignalAggregator(7500, { \"total_market\": [BTC, ETH, XRP, LTC, EOS, NEO] })\n",
    "(warmup_data_min, warmup_signals_min), data_min_ = prep_warmup_data(data_min, signal_aggregator_min, 7500)\n",
    "print(warmup_signals_min.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from trader.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, Emse, HoltEma\n",
    "from trader.util import Gaussian\n",
    "from copy import deepcopy\n",
    "\n",
    "from trader.util.constants import BCH, BTC, EOS, ETH, LTC, NEO, XRP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import warnings\n",
    "\n",
    "def bhattacharyya_multi(a, b):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of Bhattacharyya distance for 1-d Gaussians.\n",
    "    Formula from https://en.wikipedia.org/wiki/Bhattacharyya_distance\n",
    "    \"\"\"\n",
    "    return (1/4) * (\n",
    "        np.log((1/4) * (a.variance / b.variance + b.variance / a.variance + 2))\n",
    "        + (a.mean - b.mean) ** 2 / (a.variance + b.variance)\n",
    "    )\n",
    "\n",
    "def intersect_with_disagreement(gaussians):\n",
    "    intersection = Gaussian.intersect(gaussians)\n",
    "    disagreement = pd.DataFrame([bhattacharyya_multi(intersection, g) for g in gaussians])\n",
    "    return Gaussian(intersection.mean, intersection.variance * (1 + disagreement.pow(2).sum()))\n",
    "\n",
    "def remove_trend(df):\n",
    "    \"\"\"Remove linear trend from the input data by subtracting the OLS.\"\"\"\n",
    "    A = np.vstack([df.index.values, np.ones(len(df.index))]).T\n",
    "    Q = np.linalg.lstsq(A, df, rcond=None)[0]\n",
    "    trend = A @ Q\n",
    "    return df - trend\n",
    "\n",
    "class KalmanFilter(Strategy):\n",
    "    '''\n",
    "    Models fairs based on correlated movements between pairs. Weights predictions by volume and\n",
    "    likelihood of cointegration.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size,\n",
    "        movement_hl,\n",
    "        trend_hl,\n",
    "        mse_hl,\n",
    "        cointegration_period,\n",
    "        maxlag,\n",
    "        warmup_signals,\n",
    "        warmup_data,\n",
    "    ):\n",
    "        self.window_size = window_size\n",
    "        self.moving_prices = HoltEma(movement_hl, trend_hl, mse_hl)\n",
    "        self.moving_err_from_prev_fair = Emse(mse_hl)\n",
    "        self.cointegration_period = cointegration_period\n",
    "        self.maxlag = maxlag\n",
    "        self.sample_counter = 0\n",
    "        self.r = None\n",
    "        self.r2 = None\n",
    "        # TODO: do some checks for length/pairs of warmup signals/outputs\n",
    "        prices = pd.concat([warmup_signals.xs(\"price\", axis=1, level=1), warmup_data.xs(\"price\", axis=1, level=1)], axis=1, sort=False)\n",
    "        volumes = pd.concat([warmup_signals.xs(\"volume\", axis=1, level=1), warmup_data.xs(\"volume\", axis=1, level=1)], axis=1, sort=False)\n",
    "        self.price_history = RingBuffer(self.window_size, dtype=(np.float64, len(prices.columns)))\n",
    "        self.price_history.extend(prices.values)\n",
    "\n",
    "        for _, p in prices.iloc[-trend_hl * 4 :].iterrows():\n",
    "            self.moving_prices.step(p)\n",
    "\n",
    "        self.moving_volumes = Ema(mse_hl, volumes.mean())\n",
    "\n",
    "        self.moving_variances = TrendEstimator(Emse(window_size / 2, (prices.diff()[1:] ** 2).mean()), prices.iloc[-1])\n",
    "\n",
    "        self.prev_fair = Gaussian(self.moving_prices.value, [1e100 for _ in prices.columns])\n",
    "\n",
    "        self.coint_f = pd.DataFrame(1, index=warmup_signals.columns.unique(0), columns=prices.columns)\n",
    "\n",
    "    # the fair combination step assumes that all signals are i.i.d. They are not (and obviously not in the case\n",
    "    # of funds). Is this a problem?\n",
    "    def tick(self, frame, signals):\n",
    "        prices = pd.concat([signals.xs(\"price\", level=1), frame.xs(\"price\", level=1)], sort=False)\n",
    "        volumes = pd.concat([signals.xs(\"volume\", level=1), frame.xs(\"volume\", level=1)], sort=False)\n",
    "        input_names = prices.index\n",
    "        signal_names = signals.index.unique(0)\n",
    "        if self.price_history is None:\n",
    "            # TODO: will be buggy\n",
    "            self.price_history = RingBuffer(\n",
    "                self.window_size, dtype=(np.float64, len(input_names)))\n",
    "            # duplicate the first value so we can compute the most recent movement without having to check\n",
    "            self.price_history.append(prices)\n",
    "\n",
    "        self.price_history.append(prices)\n",
    "\n",
    "        price_history = pd.DataFrame(self.price_history, columns=input_names)\n",
    "\n",
    "        moving_prices = self.moving_prices.step(prices)\n",
    "        moving_volumes = self.moving_volumes.step(volumes)\n",
    "        stddev = np.sqrt(self.moving_variances.step(prices))\n",
    "\n",
    "        if len(self.price_history) < self.window_size or not self.moving_prices.ready:\n",
    "            return Gaussian(pd.Series([]),[])\n",
    "\n",
    "        if self.prev_fair is None:\n",
    "            self.prev_fair = Gaussian(moving_prices, [1e100 for _ in input_names])\n",
    "            \n",
    "        if self.coint_f is None:\n",
    "            self.coint_f = pd.DataFrame(1, index=signal_names, columns=input_names)\n",
    "            \n",
    "        # The moving average is already trend-compenstated, so we remove trend from the data.\n",
    "        # price_history = remove_trend(price_history)\n",
    "\n",
    "        # calculate p values for pair cointegration\n",
    "        if self.sample_counter == 0:\n",
    "            for i in signal_names:\n",
    "                for j in input_names:\n",
    "                    # ignore collinearity warning\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\")\n",
    "                        p = coint(price_history[i], price_history[j], trend='ct', maxlag=self.maxlag, autolag=None)[1]\n",
    "                        self.coint_f.loc[i,j] = 1 + p * p * p * 15625 # .04 -> 2, .05 -> ~3, .1 -> 15.625\n",
    "            self.r = price_history.corr().loc[signal_names]\n",
    "            self.r2 = self.r ** 2\n",
    "            \n",
    "        self.sample_counter = (self.sample_counter - 1) % self.cointegration_period\n",
    "        \n",
    "        correlated_slopes = self.r.mul(stddev, axis=1).div(stddev[signal_names], axis=0)\n",
    "        log_volume = np.log1p(moving_volumes)\n",
    "        # ideally use mkt cap instead of volume?\n",
    "        volume_f = pd.DataFrame(log_volume[np.newaxis, :] - log_volume[signal_names][:, np.newaxis], index=signal_names, columns=input_names)\n",
    "        volume_f = (volume_f * (volume_f > 0) + 1) ** 2\n",
    "#         print(volume_f)\n",
    "        \n",
    "        delta = signals.xs(\"price\",level=1) - moving_prices[signal_names]\n",
    "        fair_delta_means = correlated_slopes.mul(delta, axis=0)\n",
    "        delta_vars = self.moving_prices.mse\n",
    "        correlated_delta_vars = np.square(correlated_slopes).mul(delta_vars[signal_names], axis=0)\n",
    "        fair_delta_vars = (correlated_delta_vars + (1-self.r2) * self.coint_f * delta_vars).mul(volume_f, axis=0)\n",
    "        fair_deltas = [Gaussian(fair_delta_means.loc[i], fair_delta_vars.loc[i]) for i in signal_names]\n",
    "        fair_delta = intersect_with_disagreement(fair_deltas)\n",
    "        absolute_fair = fair_delta + moving_prices\n",
    "        \n",
    "        step = prices - (self.prev_fair.mean + self.moving_prices.trend)\n",
    "        step_vars = self.moving_err_from_prev_fair.step(step)\n",
    "        fair_step_means = correlated_slopes.mul(step[signal_names], axis=0)\n",
    "        correlated_step_vars = np.square(correlated_slopes).mul(step_vars[signal_names], axis=0)\n",
    "        fair_step_vars = (correlated_step_vars + (1-self.r2) * self.coint_f * step_vars).mul(volume_f, axis=0)\n",
    "        fair_steps = [Gaussian(fair_step_means.loc[i], fair_step_vars.loc[i]) for i in signal_names]\n",
    "        fair_step = intersect_with_disagreement(fair_steps)\n",
    "        relative_fair = fair_step + self.prev_fair + self.moving_prices.trend\n",
    "        \n",
    "        fair = intersect_with_disagreement([absolute_fair, relative_fair])\n",
    "#         print(\"\\n\\nASDSA\")\n",
    "#         print(frame[\"price\"])\n",
    "#         print(absolute_fair)\n",
    "#         print(relative_fair)\n",
    "#         print(fair)\n",
    "#         assert(0)\n",
    "        self.prev_fair = fair\n",
    "        return fair[frame.index.unique(0)]\n",
    "\n",
    "print(\"Initializing components\")\n",
    "\n",
    "analyze(run(\n",
    "    KalmanFilter(7500, 90, 3000, 1440, 60, 120, warmup_signals_min, warmup_data_min),\n",
    "    ExecutionStrategy(1000, 1440, 2, 4, 10, 20, warmup_data_min),\n",
    "    data_min_.head(2000),\n",
    "    deepcopy(signal_aggregator_min),\n",
    "    fees=0.00075,\n",
    "))\n",
    "\n",
    "# analyze(run(\n",
    "#     KalmanFilter(500, 12, 256, 500, 32, 8, warmup_signals_15min, warmup_data_15min),\n",
    "#     ExecutionStrategy(1000, 500, 1, 3, 3, 10, warmup_data_15min),\n",
    "#     data_15min_,\n",
    "#     deepcopy(signal_aggregator_15min),\n",
    "#     fees=0.00075,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "import pickle\n",
    "\n",
    "from research.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, Gaussian\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from trader.util.linalg import orthogonal_projection, hyperplane_projection\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def johansen(P, maxlag):\n",
    "    mean = P.mean()\n",
    "    c = coint_johansen(P / mean - 1, det_order=-1, k_ar_diff=maxlag)\n",
    "    significant_results = (c.lr1 > c.cvt[:,1]) * (c.lr2 < c.cvm[:,2])\n",
    "    Q = pd.DataFrame(c.evec[:,significant_results].T, columns=P.columns)\n",
    "    return mean, Q.div(Q.apply(np.linalg.norm, axis=1), axis=0)\n",
    "\n",
    "# TODO: reimplement with vector autoregression test, similar to johansen internals.\n",
    "def test_coint(P, mean_train, q, maxlag):\n",
    "    x = (P / mean_train - 1) @ q\n",
    "    return adfuller(x, regression=\"nc\", maxlag=maxlag, autolag=None)[1]\n",
    "\n",
    "def cosine_similar_to_any(Q, x):\n",
    "    for q in Q:\n",
    "        if distance.cosine(q, x) < 0.1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class LiveCointegrator(Strategy):\n",
    "    def __init__(self, train_size, validation_size, cointegration_period, maxlag):\n",
    "        self.window_size = train_size + validation_size\n",
    "        self.train_size = train_size\n",
    "        self.validation_size = validation_size\n",
    "        self.cointegration_period = cointegration_period\n",
    "        self.maxlag = maxlag\n",
    "        self.sample_counter = 0\n",
    "        self.price_history = None\n",
    "        self.Q = None\n",
    "        self.base_prices = None\n",
    "#         self.covs = None\n",
    "        self.base_cov = None\n",
    "        self.prev_fair = None\n",
    "        self.coints = []\n",
    "\n",
    "    def step(self, frame):\n",
    "        prices = frame[\"price\"]\n",
    "        \n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(self.window_size, dtype=(np.float64, len(prices.index)))\n",
    "            \n",
    "        if self.prev_fair is None:\n",
    "            self.prev_fair = self.null_estimate(prices)\n",
    "            \n",
    "        self.price_history.append(prices)\n",
    "        \n",
    "        if len(self.price_history) < self.window_size:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        if self.sample_counter == 0:\n",
    "            P = pd.DataFrame(self.price_history, columns=prices.index)\n",
    "            P_train = P.iloc[:self.train_size]\n",
    "            P_val = P.iloc[self.train_size:]\n",
    "            mean, candidates = johansen(P_train, self.maxlag)\n",
    "            self.base_prices = mean\n",
    "            new_coints = [q for q in candidates.values if test_coint(P_val, mean, q, self.maxlag) < .05]\n",
    "            old_coints = [q for q in self.coints if test_coint(P_val, mean, q, self.maxlag) < .05 and not cosine_similar_to_any(new_coints, q)]\n",
    "            self.coints = new_coints + old_coints\n",
    "            self.base_cov = (P / P.mean()).cov()\n",
    "            \n",
    "        self.sample_counter -= 1\n",
    "        self.sample_counter %= self.cointegration_period\n",
    "        \n",
    "        if not self.coints:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        fair_means = [hyperplane_projection(prices / self.base_prices - 1, q) for q in self.coints]\n",
    "        fair = Gaussian.intersect([(Gaussian(mean, self.base_cov) + 1) * self.base_prices for mean in fair_means])\n",
    "        \n",
    "#         step_fair_means = [hyperplane_projection(prices / self.prev_fair.mean - 1, q) for q in self.coints]\n",
    "#         step_fair = Gaussian.intersect([Gaussian(mean, self.base_cov) * self.prev_fair.mean for mean in step_fair_means])\n",
    "        \n",
    "#         fair = fair & (self.prev_fair + step_fair)\n",
    "        self.prev_fair = fair\n",
    "        return fair\n",
    "\n",
    "analyze(run(LiveCointegrator(train_size=500, validation_size = 100, cointegration_period=64, maxlag=8), data_15min, fees=0.001, min_edge=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "import pickle\n",
    "\n",
    "from research.strategy.base import Strategy\n",
    "from trader.util.stats import Ema, Gaussian\n",
    "from trader.util.linalg import orthogonal_projection, hyperplane_projection\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "def plot_gaussian(x, y, gaussian):\n",
    "    z = gaussian.pdf(np.dstack(np.meshgrid(x,y)))\n",
    "    plt.pcolormesh(x, y, z)\n",
    "    plt.show()\n",
    "\n",
    "class LivePairCointegrator(Strategy):\n",
    "    def __init__(self, window_size, cointegration_frequency=4):\n",
    "        self.window_size = window_size\n",
    "        self.cointegration_period = window_size // cointegration_frequency\n",
    "        self.sample_counter = 0\n",
    "        self.price_history = None\n",
    "        self.prediction_covs = {}\n",
    "\n",
    "    def step(self, frame):\n",
    "        prices = frame[\"price\"]\n",
    "        \n",
    "        if self.price_history is None:\n",
    "            self.price_history = RingBuffer(self.window_size, dtype=(np.float64, len(prices.index)))\n",
    "            \n",
    "        self.price_history.append(prices)\n",
    "        \n",
    "        if len(self.price_history) < self.window_size:\n",
    "            return self.null_estimate(prices)\n",
    "        \n",
    "        df = pd.DataFrame(self.price_history, columns=prices.index)\n",
    "        self.mean = df.mean()\n",
    "        deltas = df / self.mean - 1\n",
    "        stddev = deltas.std() + 1e-100\n",
    "        regression_slope = deltas.corr().mul(stddev, axis=1).div(stddev, axis=0)\n",
    "        cov = deltas.cov()\n",
    "        \n",
    "        if self.sample_counter == 0:\n",
    "            \n",
    "            for pair_i in prices.index:\n",
    "                for pair_j in prices.index:\n",
    "                    if pair_i >= pair_j:\n",
    "                        continue\n",
    "                    deltas_ij = deltas[[pair_i, pair_j]]\n",
    "                    regression_vector = [regression_slope.loc[pair_i][pair_j], -1]\n",
    "                    residuals = orthogonal_projection(deltas_ij, regression_vector)\n",
    "                    cov_resid = residuals.cov()\n",
    "                    cov_pred = (deltas_ij - residuals).cov()\n",
    "                    print(cov_pred)\n",
    "                    # We can calculate the cointegration p-value more efficiently by reusing\n",
    "                    # the residuals found via the orthogonal projection but getting it right is\n",
    "                    # tricky. So we just do it the dumb way\n",
    "                    p = coint(deltas[pair_j], deltas[pair_i], trend='nc', maxlag=0, autolag=None)[1]\n",
    "#                     # regularize prediction covariance by using the raw price covariance shape with the \n",
    "#                     # prediction covariance slope\n",
    "#                     cov_sam = deltas_ij.cov()\n",
    "#                     w_pred, v_pred = np.linalg.eigh(cov_pred)\n",
    "#                     w_sam, v_sam = np.linalg.eigh(cov_sam)\n",
    "#                     eigenvalue_ratio_sam = w_sam[1] / w_sam[0]\n",
    "#                     w_reg = w_pred[1] * np.array([eigenvalue_ratio_sam, 1])\n",
    "#                     cov_reg = v_pred @ np.diag(w_reg) @ v_pred.T\n",
    "#                     # discount predictions based on p-value\n",
    "#                     cov_reg *= max(1, p * p * 900) # min(1e4, np.cosh(16 * p))\n",
    "#                     cov_reg = pd.DataFrame(cov_reg, index=deltas_ij.columns, columns=deltas_ij.columns)\n",
    "                    cov_reg = cov_pred + max(1, p * p * 900) * cov_resid\n",
    "                    self.prediction_covs[pair_i, pair_j] = cov_reg\n",
    "                    self.prediction_covs[pair_j, pair_i] = cov_reg.loc[::-1, ::-1]\n",
    "                    \n",
    "                    # uncomment and run this if you want to see what the prediction covariances look like\n",
    "                    \n",
    "                    prediction = Gaussian(pd.Series(0, index=deltas_ij.columns), cov_resid)\n",
    "                    base = Gaussian(pd.Series(0, index=deltas_ij.columns),  cov_pred)\n",
    "                    regularized = Gaussian(pd.Series(0, index=deltas_ij.columns), cov_reg)\n",
    "                    xrange = deltas[pair_i].max() - deltas[pair_i].min()\n",
    "                    yrange = deltas[pair_j].max() - deltas[pair_j].min()\n",
    "                    x = np.linspace(-xrange, xrange)\n",
    "                    y = np.linspace(-yrange, yrange)\n",
    "                    plot_gaussian(x, y, prediction)\n",
    "                    plot_gaussian(x, y, base)\n",
    "                    plot_gaussian(x, y, regularized)\n",
    "            \n",
    "        self.sample_counter -= 1\n",
    "        self.sample_counter %= self.cointegration_period\n",
    "        \n",
    "        fairs = []\n",
    "        delta = prices / self.mean - 1\n",
    "        cov = deltas.cov()\n",
    "        for pair_i in prices.index:\n",
    "            for pair_j in prices.index:\n",
    "                if pair_i >= pair_j:\n",
    "                    continue\n",
    "                regression_vector = [regression_slope.loc[pair_i][pair_j], -1]\n",
    "#                 residuals = orthogonal_projection(deltas[[pair_i, pair_j]], regression_vector)\n",
    "                fair_delta_mean = hyperplane_projection(delta[[pair_i, pair_j]], regression_vector)\n",
    "                fair_cov = self.prediction_covs[pair_i, pair_j]\n",
    "                fair = Gaussian(fair_delta_mean, fair_cov)\n",
    "#                 fair = Gaussian(fair_delta_mean, cov.loc[[pair_i, pair_j], [pair_i, pair_j]])\n",
    "#                 scale = fair.z_score(delta[[pair_i, pair_j]])\n",
    "#                 fair = Gaussian(fair.mean, fair.covariance * scale)\n",
    "#                 print(prices)\n",
    "#                 print(fair)\n",
    "#                 x = np.linspace(deltas[pair_i].min(), deltas[pair_i].max()) - deltas[pair_i].mean()\n",
    "#                 y = np.linspace(deltas[pair_j].min(), deltas[pair_j].max()) - deltas[pair_j].mean()\n",
    "#                 plot_gaussian(x, y, fair)\n",
    "#                 plot_gaussian(x, y, Gaussian(pd.Series(0, index=[pair_i, pair_j]), residuals.cov()))\n",
    "#                 plot_gaussian(x, y, Gaussian(pd.Series(0, index=[pair_i, pair_j]), deltas[[pair_i, pair_j]].cov()))\n",
    "                fairs.append(fair)\n",
    "#         print(fairs)\n",
    "        fair = (Gaussian.intersect(fairs) + 1) * self.mean\n",
    "#         print(prices)\n",
    "#         print(fair)\n",
    "#         x = np.linspace(df['BTC_USDT'].min(), df['BTC_USDT'].max())\n",
    "#         y = np.linspace(df['ETH_USDT'].min(), df['ETH_USDT'].max())\n",
    "#         plot_gaussian(x, y, fair[['BTC_USDT', 'ETH_USDT']])\n",
    "#         print(fair / prices - 1)\n",
    "        return fair\n",
    "\n",
    "analyze(run(LivePairCointegrator(window_size=96), data_15min.tail(1500), fees=0.001, min_edge=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CointegratorStrategy(cointegration_window_size = 16), data_15min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(KalmanFilterStrategy(\n",
    "    correlation_window_size = 480,\n",
    "    movement_half_life = 1\n",
    "), tail_data(data_min, 10000), fees = 0.002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strategy import CombinedStrategy\n",
    "\n",
    "analyze(run(CombinedStrategy([\n",
    "    KalmanFilterStrategy(correlation_window_size = 60, movement_half_life = 3),\n",
    "    CointegratorStrategy(cointegration_window_size = 16)\n",
    "]), data_15min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CointegratorStrategy(cointegration_window_size = 512), data_5min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(KalmanFilterStrategy(correlation_window_size = 165, movement_half_life = 70), data_5min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(run(CombinedStrategy([\n",
    "    KalmanFilterStrategy(correlation_window_size = 16, movement_half_life = 8),\n",
    "    CointegratorStrategy(cointegration_window_size = 64)\n",
    "]), tail_data(data_min, 1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def find_best_window_sizes(data, n):\n",
    "    points = []\n",
    "    best = None\n",
    "    best_ror = 0\n",
    "    for _ in range(n):\n",
    "        movement_half_life = random.expovariate(1) * 15\n",
    "#         window_ratio = random.uniform(1, 10)\n",
    "#         window_size = max(3, int(movement_half_life * window_ratio))\n",
    "#         movement_half_life = 2\n",
    "        window_size = int(random.expovariate(1) * 100) + 400\n",
    "#         window_size = int(random.expovariate(1) * 30) + 3\n",
    "#         window_size = 4\n",
    "#         window_size = 32\n",
    "        print('Trying window_size: {0} and half_life: {1}'.format(window_size, movement_half_life))\n",
    "        ror = analyze(run(KalmanFilter(window_size, movement_half_life), data, fees = 0.002), plot=False)\n",
    "        print('  RoR: {0}'.format(ror))\n",
    "        point = { 'window_size': window_size, 'half_life': movement_half_life, 'RoR': ror }\n",
    "        points.append(point)\n",
    "        if ror > best_ror:\n",
    "            best = point\n",
    "            best_ror = ror\n",
    "    print('Best found:')\n",
    "    print(best)\n",
    "    pd.DataFrame(points).plot.scatter('window_size', 'half_life', c='RoR', colormap='jet')\n",
    "    \n",
    "find_best_window_sizes(tail_data(data_min, 1500), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
